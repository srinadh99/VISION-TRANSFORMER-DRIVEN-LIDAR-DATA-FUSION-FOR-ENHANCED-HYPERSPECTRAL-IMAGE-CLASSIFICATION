{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d61d35-9c96-4432-a636-7cd0489dd06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time \n",
    "\n",
    "from einops import rearrange\n",
    "import torch.optim as optim\n",
    "from operator import truediv\n",
    "# import torchvision\n",
    "# from torchvision import datasets, transforms\n",
    "from scipy import io\n",
    "import torch.utils.data\n",
    "import scipy.io as sio\n",
    "# import mat73\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "745f0f98-573c-4245-976e-859021ba95c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "datasetname = 'UHouston'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a99e0f-c9cc-4a57-999d-bbc2a20cdf96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsi_2013_data shape: (349, 1905, 144)\n",
      "Lidar_2013_data shape: (349, 1905, 1)\n",
      "gt_2013_data.shape: (349, 1905)\n"
     ]
    }
   ],
   "source": [
    "hsi_2013_data=sio.loadmat('./'+str(datasetname)+'/2013_IEEE_GRSS_DF_Contest_CASI.mat')['HSI_data']\n",
    "print('hsi_2013_data shape:', hsi_2013_data.shape)\n",
    "\n",
    "# Loader Lidar  data\n",
    "# import mat73\n",
    "lidar_2013_data = sio.loadmat('./'+str(datasetname)+'/2013_IEEE_GRSS_DF_Contest_LiDAR.mat')['LiDAR_data']\n",
    "\n",
    "print('Lidar_2013_data shape:', lidar_2013_data.shape)\n",
    "\n",
    "#Load ground truth labels\n",
    "gt_2013_data=sio.loadmat('./'+str(datasetname)+'/GRSS2013.mat')['name']\n",
    "print('gt_2013_data.shape:', gt_2013_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "192b18c3-a4c5-4937-afcf-1e2afcca97bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'class_name': 'Healthy grass', 'training_sample': 198, 'test_sample': 1053, 'total_samples': 1251}, 2: {'class_name': 'Stressed grass', 'training_sample': 190, 'test_sample': 1064, 'total_samples': 1254}, 3: {'class_name': 'Synthetic grass', 'training_sample': 192, 'test_sample': 505, 'total_samples': 697}, 4: {'class_name': 'Trees', 'training_sample': 188, 'test_sample': 1058, 'total_samples': 1244}, 5: {'class_name': 'Soil', 'training_sample': 186, 'test_sample': 1056, 'total_samples': 1242}, 6: {'class_name': 'Water', 'training_sample': 182, 'test_sample': 141, 'total_samples': 325}, 7: {'class_name': 'Residential', 'training_sample': 196, 'test_sample': 1072, 'total_samples': 1268}, 8: {'class_name': 'Commercial', 'training_sample': 191, 'test_sample': 1053, 'total_samples': 1244}, 9: {'class_name': 'Road', 'training_sample': 193, 'test_sample': 1059, 'total_samples': 1252}, 10: {'class_name': 'Highway', 'training_sample': 191, 'test_sample': 1036, 'total_samples': 1227}, 11: {'class_name': 'Railway', 'training_sample': 181, 'test_sample': 1054, 'total_samples': 1235}, 12: {'class_name': 'Parking lot 1', 'training_sample': 192, 'test_sample': 1041, 'total_samples': 1233}, 13: {'class_name': 'Parking lot 2', 'training_sample': 184, 'test_sample': 285, 'total_samples': 469}, 14: {'class_name': 'Tennis court', 'training_sample': 181, 'test_sample': 247, 'total_samples': 428}, 15: {'class_name': 'Running track', 'training_sample': 187, 'test_sample': 473, 'total_samples': 660}}\n"
     ]
    }
   ],
   "source": [
    "class_info = [(1, \"Healthy grass\", 'training_sample', 198, 'test_sample', 1053,  'total', 1251),\n",
    "    (2, \"Stressed grass\",'training_sample', 190, 'test_sample', 1064,  'total', 1254),\n",
    "    (3, \"Synthetic grass\", 'training_sample', 192, 'test_sample', 505,  'total', 697),\n",
    "    (4, \"Trees\", 'training_sample', 188, 'test_sample', 1058,  'total', 1244),\n",
    "    (5, \"Soil\",'training_sample', 186, 'test_sample', 1056,  'total', 1242),\n",
    "    (6, \"Water\", 'training_sample', 182, 'test_sample', 141,  'total', 325),\n",
    "    (7, \"Residential\", 'training_sample', 196, 'test_sample', 1072,  'total', 1268),\n",
    "    (8, \"Commercial\", 'training_sample', 191, 'test_sample', 1053,  'total', 1244),\n",
    "    (9, \"Road\", 'training_sample', 193, 'test_sample', 1059,  'total', 1252),\n",
    "    (10, \"Highway\", 'training_sample', 191, 'test_sample', 1036,  'total', 1227),\n",
    "    (11, \"Railway\", 'training_sample', 181, 'test_sample', 1054,  'total', 1235),\n",
    "    (12, \"Parking lot 1\", 'training_sample', 192, 'test_sample', 1041,  'total', 1233),\n",
    "    (13, \"Parking lot 2\", 'training_sample', 184, 'test_sample',285,  'total', 469),\n",
    "    (14, \"Tennis court\",'training_sample', 181, 'test_sample', 247,  'total', 428),\n",
    "    (15, \"Running track\", 'training_sample', 187, 'test_sample', 473,  'total', 660)]\n",
    "\n",
    "# Create a dictionary to store class number, class name, and class samples\n",
    "class_dict = {class_number: {\"class_name\": class_name,\n",
    "                             'training_sample': training_sample,\n",
    "                             'test_sample': test_sample,\n",
    "                             \"total_samples\": total}\n",
    "              for class_number, class_name, _, training_sample, _, test_sample, _, total in class_info}\n",
    "\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b37a86-1214-4a86-9d23-47a71c86ddc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsi_samples shape: (15029, 9, 9, 144)\n",
      "lidar_samples shape: (15029, 9, 9, 1)\n",
      "labels shape: (15029,)\n"
     ]
    }
   ],
   "source": [
    "# Define patch size and stride\n",
    "patch_size = 9\n",
    "stride = 1\n",
    "\n",
    "# Create an empty list to store patches and labels\n",
    "hsi_samples = []\n",
    "lidar_samples = []\n",
    "labels = []\n",
    "\n",
    "# Initialize a dictionary to store class count\n",
    "class_count = {i: 0 for i in class_dict.keys()}\n",
    "\n",
    "# Function to check if all classes have the required number of samples\n",
    "def all_classes_completed(class_count, class_dict):\n",
    "    return all(class_count[class_num] == class_dict[class_num][\"total_samples\"] for class_num in class_dict.keys())\n",
    "\n",
    "while not all_classes_completed(class_count, class_dict):\n",
    "    # Loop through the ground truth data\n",
    "    for label in class_dict.keys():\n",
    "        # Get the coordinates of the ground truth pixels\n",
    "        #coords = np.argwhere((gt_2013_data == label) & (mask > 0))\n",
    "        coords = np.argwhere(gt_2013_data == label)\n",
    "\n",
    "        # Shuffle the coordinates to randomize the patch extraction\n",
    "        np.random.shuffle(coords)\n",
    "\n",
    "        for coord in coords:\n",
    "            i, j = coord\n",
    "            # Calculate the patch indices\n",
    "            i_start, i_end = i - patch_size // 2, i + patch_size // 2 + 1\n",
    "            j_start, j_end = j - patch_size // 2, j + patch_size // 2 + 1\n",
    "\n",
    "            # Check if the indices are within the bounds of the HSI data\n",
    "            if i_start >= 0 and i_end <= hsi_2013_data.shape[0] and j_start >= 0 and j_end <= hsi_2013_data.shape[1]:\n",
    "                # Extract the patch\n",
    "                hsi_patch = hsi_2013_data[i_start:i_end, j_start:j_end, :]\n",
    "\n",
    "                # Extract the LiDAR patch\n",
    "                lidar_patch = lidar_2013_data[i_start:i_end, j_start:j_end, :]\n",
    "\n",
    "                # If the class count is less than the required samples\n",
    "                if class_count[label] < class_dict[label][\"total_samples\"]:\n",
    "                    # Append the patch and its label to the list\n",
    "                    hsi_samples.append(hsi_patch)\n",
    "                    lidar_samples.append(lidar_patch)\n",
    "                    labels.append(label)\n",
    "                    class_count[label] += 1\n",
    "\n",
    "                    # If all classes have the required number of samples, exit the loop\n",
    "                    if all_classes_completed(class_count, class_dict):\n",
    "                        break\n",
    "\n",
    "# Convert the list of patches and labels into arrays\n",
    "hsi_samples = np.array(hsi_samples)\n",
    "lidar_samples = np.array(lidar_samples)\n",
    "labels = np.array(labels)\n",
    "print('hsi_samples shape:', hsi_samples.shape)\n",
    "print('lidar_samples shape:', lidar_samples.shape)\n",
    "print('labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f30b9f73-9ba7-4de4-a790-0e97020e7084",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsi_training_samples shape: (2832, 9, 9, 144)\n",
      "lidar_training_samples shape: (2832, 9, 9, 1)\n",
      "training_labels shape: (2832,)\n",
      "hsi_test_samples shape: (12197, 9, 9, 144)\n",
      "lidar_test_samples shape: (12197, 9, 9, 1)\n",
      "test_labels shape: (12197,)\n"
     ]
    }
   ],
   "source": [
    "#Avoid overlap of train and test\n",
    "# Extracting training samples\n",
    "hsi_training_samples, lidar_training_samples, training_labels = [], [], []\n",
    "used_indices = []  # To keep track of indices already taken for training samples\n",
    "\n",
    "for label, class_data in class_dict.items():\n",
    "    # Get indices of the current class\n",
    "    class_indices = np.where(labels == label)[0]\n",
    "\n",
    "    # Randomly shuffle the indices\n",
    "    np.random.shuffle(class_indices)\n",
    "\n",
    "    # Take the required number of training samples\n",
    "    train_indices = class_indices[:class_data[\"training_sample\"]]\n",
    "    used_indices.extend(train_indices)  # Add these to the used_indices list\n",
    "\n",
    "    # Append training samples\n",
    "    hsi_training_samples.extend(hsi_samples[train_indices])\n",
    "    lidar_training_samples.extend(lidar_samples[train_indices])\n",
    "    training_labels.extend(labels[train_indices])\n",
    "\n",
    "# Extracting test samples\n",
    "hsi_test_samples, lidar_test_samples, test_labels = [], [], []\n",
    "\n",
    "for label, class_data in class_dict.items():\n",
    "    class_indices = np.where(labels == label)[0]\n",
    "\n",
    "    # Exclude indices which were used for training\n",
    "    test_indices = np.setdiff1d(class_indices, used_indices)\n",
    "\n",
    "    # Append test samples\n",
    "    hsi_test_samples.extend(hsi_samples[test_indices])\n",
    "    lidar_test_samples.extend(lidar_samples[test_indices])\n",
    "    test_labels.extend(labels[test_indices])\n",
    "\n",
    "# Convert lists back to numpy arrays\n",
    "hsi_training_samples = np.array(hsi_training_samples)\n",
    "lidar_training_samples = np.array(lidar_training_samples)\n",
    "training_labels = np.array(training_labels)\n",
    "\n",
    "hsi_test_samples = np.array(hsi_test_samples)\n",
    "lidar_test_samples = np.array(lidar_test_samples)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Print shapes to verify\n",
    "print('hsi_training_samples shape:', hsi_training_samples.shape)\n",
    "print('lidar_training_samples shape:', lidar_training_samples.shape)\n",
    "print('training_labels shape:', training_labels.shape)\n",
    "\n",
    "print('hsi_test_samples shape:', hsi_test_samples.shape)\n",
    "print('lidar_test_samples shape:', lidar_test_samples.shape)\n",
    "print('test_labels shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0649a9e6-b183-4e7a-9e5a-9bd018fccaf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsi_train_samples shape: (2832, 9, 9, 144)\n",
      "lidar_train_samples shape: (2832, 9, 9, 1)\n",
      "train_labels shape: (2832,)\n",
      "hsi_test_samples shape: (12197, 9, 9, 144)\n",
      "lidar_test_samples shape: (12197, 9, 9, 1)\n",
      "y_test shape: (12197,)\n"
     ]
    }
   ],
   "source": [
    "hsi_train=hsi_training_samples\n",
    "lidar_train=lidar_training_samples\n",
    "y_train=training_labels-1\n",
    "print('hsi_train_samples shape:', hsi_train.shape)\n",
    "print('lidar_train_samples shape:', lidar_train.shape)\n",
    "print('train_labels shape:', y_train.shape)\n",
    "hsi_test=hsi_test_samples\n",
    "lidar_test=lidar_test_samples\n",
    "y_test=test_labels-1\n",
    "print('hsi_test_samples shape:', hsi_test.shape)\n",
    "print('lidar_test_samples shape:', lidar_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb897ac2-305a-4742-baca-f74b35fec1c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2832, 81, 144, 1)\n",
      "(2832, 81, 1, 1)\n",
      "(12197, 81, 144, 1)\n",
      "(12197, 81, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "hsi_train = hsi_train.reshape(hsi_train.shape[0], patch_size*patch_size, hsi_train.shape[-1], 1)\n",
    "lidar_train = lidar_train.reshape(lidar_train.shape[0], patch_size*patch_size, lidar_train.shape[-1], 1)\n",
    "\n",
    "hsi_test = hsi_test.reshape(hsi_test.shape[0], patch_size*patch_size, hsi_test.shape[-1], 1)\n",
    "lidar_test = lidar_test.reshape(lidar_test.shape[0], patch_size*patch_size, lidar_test.shape[-1], 1)\n",
    "\n",
    "print(hsi_train.shape)\n",
    "print(lidar_train.shape)\n",
    "\n",
    "print(hsi_test.shape)\n",
    "print(lidar_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0b8728e-9912-4052-aa08-0c71d827844c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "1\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "filename = 'UH_ViT_CA_LF_CLS_HSI_LiDAR'\n",
    "NC = hsi_train.shape[2]\n",
    "NCLiDAR = lidar_train.shape[2]\n",
    "n_classes = len(np.unique(training_labels))\n",
    "print(NC)\n",
    "print(NCLiDAR)\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a8e6afd-b8f9-40c7-bd18-729678e3eb60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_confusion_matrix(y_test, y_pred, name, c, plt_name):\n",
    "    # print(plt_name)\n",
    "    df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(c), range(c))\n",
    "        \n",
    "    if name == 'UTrento':\n",
    "        df_cm.columns = ['Buildings', 'Woods', 'Roads', 'Apples', 'Ground', 'Vineyard']\n",
    "        df_cm = df_cm.rename(index={0: 'Buildings', 1: 'Woods', 2: 'Roads', 3: 'Apples', 4: 'Ground', 5: 'Vineyard'})\n",
    "    \n",
    "    elif name == 'UHouston':\n",
    "        df_cm.columns = ['Healthy grass', 'Stressed grass', 'Synthetic grass', 'Trees', 'Soil', 'Water',  'Residential', 'Commercial', 'Road', 'Highway',\n",
    "                        'Railway', 'Parking Lot 1', 'Parking Lot 2', 'Tennis Court', 'Running Track']    \n",
    "        df_cm = df_cm.rename(index={0:'Healthy grass', 1:'Stressed grass', 2:'Synthetic grass', 3:'Trees', 4:'Soil', 5:'Water',  6:'Residential', 7:'Commercial', 8:'Road', 9:'Highway',\n",
    "                        10:'Railway', 11:'Parking Lot 1', 12:'Parking Lot 2', 13:'Tennis Court', 14:'Running Track'})\n",
    "    \n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted'\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.set(font_scale=1.2)  # for label size\n",
    "    heatmap = sns.heatmap(df_cm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 14}, fmt='g', cbar=False)\n",
    "\n",
    "    heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=45, ha='right', fontsize=12)\n",
    "    heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0, fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{plt_name}.png', format='png')\n",
    "    plt.close()\n",
    "\n",
    "def AA_andEachClassAccuracy(confusion_matrix):\n",
    "    counter = confusion_matrix.shape[0]\n",
    "    list_diag = np.diag(confusion_matrix)\n",
    "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
    "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
    "    average_acc = np.mean(each_acc)\n",
    "    return each_acc, average_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ee3f2a8-6010-468c-8dc9-ea5780d1f3b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, num_patches, dim, dropout):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_patches = num_patches\n",
    "        self.dim = dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels,  # Using num_patches instead of in_channels\n",
    "            kernel_size=(1,1),\n",
    "            stride=1,\n",
    "        )\n",
    "\n",
    "        self.linear_proj = nn.Linear(in_channels, dim)\n",
    "        self.norm = nn.LayerNorm(dim)  # Adjusted to apply to each embedding vector\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout)  # Renamed to avoid conflict with attribute name\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        x = self.proj(x)  # Apply convolution\n",
    "        \n",
    "        x = x.view(B, C, -1)  # Shape: [B, C, H*W]\n",
    "        x = x.permute(0, 2, 1)  # Change to [B, spatial_dim, C]\n",
    "        \n",
    "        x = self.linear_proj(x)  # Apply linear projection correctly\n",
    "        \n",
    "        x = self.norm(x)  # Normalize each embedding vector\n",
    "\n",
    "        x = torch.cat((self.cls_token.expand(B, -1, -1), x), dim=1)\n",
    "        x = x + self.pos_embedding\n",
    "        x = self.dropout_layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class HSISelfAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dim_head, dropout):\n",
    "        super(HSISelfAttention, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.dim_head = dim_head\n",
    "        self.num_heads = n_heads\n",
    "        self.sqrt_dim_head = math.sqrt(self.dim_head)\n",
    "        self.dropout_layer = nn.Dropout(dropout)  # Renamed to avoid conflict with attribute name\n",
    "\n",
    "        # Layer normalization before Q, K, V projections\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, dim_head * n_heads, bias=False)\n",
    "        self.to_k = nn.Linear(dim, dim_head * n_heads, bias=False)\n",
    "        self.to_v = nn.Linear(dim, dim_head * n_heads, bias=False)\n",
    "\n",
    "        self.to_out = nn.Linear(n_heads * dim_head, dim)\n",
    "        self.dropout_layer = nn.Dropout(dropout)  # This line seems duplicated, corrected below\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, _ = x.size()\n",
    "\n",
    "        x = self.norm(x)  # Apply normalization before attention\n",
    "\n",
    "        Q = self.to_q(x)\n",
    "        K = self.to_k(x)\n",
    "        V = self.to_v(x)\n",
    "        \n",
    "        Q = Q.view(B, N, self.num_heads, self.dim_head).transpose(1, 2)  # [B, num_heads, 144, dim_head]\n",
    "        K = K.view(B, N, self.num_heads, self.dim_head).transpose(1, 2)  # [B, num_heads, 144, dim_head]\n",
    "        V = V.view(B, N, self.num_heads, self.dim_head).transpose(1, 2)  # [B, num_heads, 144, dim_head]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.sqrt_dim_head\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # [B, num_heads, 144, 144]\n",
    "\n",
    "        # Apply attention to V\n",
    "        attn_output = torch.matmul(attn_weights, V)  # [B, num_heads, 144, dim_head]\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(B, -1, self.num_heads * self.dim_head)  # [B, 144, num_heads * dim_head]\n",
    "\n",
    "        attn_output = self.dropout_layer(attn_output)\n",
    "        \n",
    "        # Project back to the original embedding space\n",
    "        attn_output = self.to_out(attn_output)  # [B, 144, num_patches]\n",
    "        \n",
    "        return attn_output\n",
    "    \n",
    "class HSICrossAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dim_head, dropout):\n",
    "        super(HSICrossAttention, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.dim_head = dim_head\n",
    "        self.num_heads = n_heads\n",
    "        self.sqrt_dim_head = math.sqrt(self.dim_head)\n",
    "        self.dropout_layer = nn.Dropout(dropout)  # Renamed to avoid conflict with attribute name\n",
    "\n",
    "        # Layer normalization before Q, K, V projections\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, dim_head * n_heads, bias=False)\n",
    "        self.to_k = nn.Linear(dim, dim_head * n_heads, bias=False)\n",
    "        self.to_v = nn.Linear(dim, dim_head * n_heads, bias=False)\n",
    "\n",
    "        self.to_out = nn.Linear(n_heads * dim_head, dim)\n",
    "        self.dropout_layer = nn.Dropout(dropout)  # This line seems duplicated, corrected below\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        B, N1, _ = x1.size()\n",
    "        B, N2, _ = x2.size()\n",
    "\n",
    "        x2 = self.norm(x2)  # Apply normalization before attention\n",
    "\n",
    "        Q = self.to_q(x1)\n",
    "        K = self.to_k(x2)\n",
    "        V = self.to_v(x2)\n",
    "        \n",
    "        Q = Q.view(B, N1, self.num_heads, self.dim_head).transpose(1, 2)  # [B, num_heads, 144, dim_head]\n",
    "        K = K.view(B, N2, self.num_heads, self.dim_head).transpose(1, 2)  # [B, num_heads, 144, dim_head]\n",
    "        V = V.view(B, N2, self.num_heads, self.dim_head).transpose(1, 2)  # [B, num_heads, 144, dim_head]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.sqrt_dim_head\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # [B, num_heads, 144, 144]\n",
    "        attn_weights = attn_weights.mean(dim=2, keepdim=True)        \n",
    "        \n",
    "        # Apply attention to V\n",
    "        attn_output = torch.matmul(attn_weights, V)  # [B, num_heads, 144, dim_head]\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(B, -1, self.num_heads * self.dim_head)  # [B, 144, num_heads * dim_head]\n",
    "\n",
    "        attn_output = self.dropout_layer(attn_output)\n",
    "        \n",
    "        # Project back to the original embedding space\n",
    "        attn_output = self.to_out(attn_output)  # [B, 144, num_patches]\n",
    "        \n",
    "        return attn_output\n",
    "\n",
    "# MLP Module in the Transformer Encoder \n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim, dropout):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(dim, mlp_dim)\n",
    "        self.fc2 = nn.Linear(mlp_dim, dim)\n",
    "        self.act_fn = nn.GELU()\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Single Transformer Encoder Block that combines the Self-Attention and MLP layers     \n",
    "class BlockC(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dim_head, mlp_dim, dropout):\n",
    "        super(BlockC, self).__init__()\n",
    "        self.hidden_size = dim\n",
    "        self.hidden_dim_size = mlp_dim\n",
    "        self.attention_norm = nn.LayerNorm(dim)\n",
    "        self.ffn_norm = nn.LayerNorm(dim)\n",
    "        self.ffn = Mlp(dim, mlp_dim, dropout)\n",
    "        self.attn = HSICrossAttention(dim, n_heads, dim_head, dropout)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        h = x2\n",
    "        x2 = self.attention_norm(x2)\n",
    "        x2= self.attn(x1, x2)\n",
    "        x2 = x2 + h\n",
    "\n",
    "        h = x2\n",
    "        x2 = self.ffn_norm(x2)\n",
    "        x2 = self.ffn(x2)\n",
    "        x2 = x2 + h\n",
    "        \n",
    "        return x2\n",
    "        \n",
    "# Transformer Encoder Block with Multi-head Self-Attention repetition    \n",
    "class TransformerEncoderC(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dim_head, mlp_dim, dropout, depth):\n",
    "        super(TransformerEncoderC, self).__init__()\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            layer = BlockC(dim, n_heads, dim_head, mlp_dim, dropout)\n",
    "            self.layer.append(copy.deepcopy(layer))       \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        for layer_block in self.layer:\n",
    "            x = layer_block(x1, x2)\n",
    "            \n",
    "        encoded = self.encoder_norm(x)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "# Single Transformer Encoder Block that combines the Self-Attention and MLP layers     \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dim_head, mlp_dim, dropout):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = dim\n",
    "        self.hidden_dim_size = mlp_dim\n",
    "        self.attention_norm = nn.LayerNorm(dim)\n",
    "        self.ffn_norm = nn.LayerNorm(dim)\n",
    "        self.ffn = Mlp(dim, mlp_dim, dropout)\n",
    "        self.attn = HSISelfAttention(dim, n_heads, dim_head, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        h = x\n",
    "        \n",
    "        x = self.attention_norm(x)\n",
    "        x = self.attn(x)\n",
    "        \n",
    "        x = x + h\n",
    "\n",
    "        h = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + h\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Transformer Encoder Block with Multi-head Self-Attention repetition    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dim_head, mlp_dim, dropout, depth):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            layer = Block(dim, n_heads, dim_head, mlp_dim, dropout)\n",
    "            self.layer.append(copy.deepcopy(layer))       \n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer_block in self.layer:\n",
    "            x = layer_block(x)\n",
    "            \n",
    "        encoded = self.encoder_norm(x)\n",
    "\n",
    "        return encoded\n",
    "    \n",
    "# The Final ViT Implementation with cls token from other modalities\n",
    "class ViT_CA(nn.Module):\n",
    "    def __init__(self, in_channels=81, num_patches1=144, num_patches2=1, dim=128, n_heads=8, dim_head=64, mlp_dim=256,  depth=3, num_classes=15, dropout=0.4):\n",
    "        super(ViT_CA, self).__init__()\n",
    "        \n",
    "        self.num_patches1 = num_patches1\n",
    "        self.patch_embedding1 = PatchEmbedding(in_channels, num_patches1, dim, dropout)\n",
    "        self.patch_embedding2 = PatchEmbedding(in_channels, num_patches2, dim, dropout)\n",
    "        self.transformer = TransformerEncoder(dim, n_heads, dim_head, mlp_dim, dropout, depth-1)\n",
    "        self.transformer1 = TransformerEncoderC(dim, n_heads, dim_head, mlp_dim, dropout, depth-2)\n",
    "        self.linear_head = nn.Linear(dim, num_classes)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        x1 = self.patch_embedding1(x1)\n",
    "        \n",
    "        x2 = self.patch_embedding2(x2)\n",
    "        \n",
    "        x1 = self.transformer(x1)\n",
    "        \n",
    "        x2 = self.transformer(x2)\n",
    "        \n",
    "        x1_1 = self.transformer1(x2, x1)\n",
    "        x2_1 = self.transformer1(x1, x2)\n",
    "        \n",
    "        # x = x.mean(dim=1)\n",
    "\n",
    "        logits_hsi = self.linear_head(x1_1[:,0])\n",
    "        logits_lid = self.linear_head(x2_1[:,0])\n",
    "       \n",
    "        out = (logits_hsi + logits_lid) / 2\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cf08557-2ec8-4e64-98d9-29c4f2b6c048",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders for training, validation, and testing are ready.\n"
     ]
    }
   ],
   "source": [
    "# Convert HSI and label data to PyTorch tensors\n",
    "hsi_train_tensor = torch.from_numpy(hsi_train.astype(np.float32))  # .permute(0, 1, 2, 3) Shape: (batch, channels, height, width)\n",
    "lidar_train_tensor = torch.from_numpy(lidar_train.astype(np.float32))  # .permute(0, 1, 2, 3) Shape: (batch, channels, height, width)\n",
    "training_labels_tensor = torch.from_numpy(y_train.astype(np.int64))\n",
    "\n",
    "hsi_test_tensor = torch.from_numpy(hsi_test.astype(np.float32))  # Shape: (batch, channels, height, width)\n",
    "lidar_test_tensor = torch.from_numpy(lidar_test.astype(np.float32))  # Shape: (batch, channels, height, width)\n",
    "test_labels_tensor = torch.from_numpy(y_test.astype(np.int64))\n",
    "\n",
    "# Define a dataset class for HSI data\n",
    "class HyperspectralDataset(Dataset):\n",
    "    def __init__(self, hsi_data, lidar_data, labels):\n",
    "        self.hsi_data = hsi_data\n",
    "        self.lidar_data = lidar_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.hsi_data[idx], self.lidar_data[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HyperspectralDataset(hsi_train_tensor, lidar_train_tensor, training_labels_tensor)\n",
    "# val_dataset = HyperspectralDataset(hsi_samples_val, labels_val)\n",
    "test_dataset = HyperspectralDataset(hsi_test_tensor, lidar_test_tensor, test_labels_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "print('DataLoaders for training, validation, and testing are ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebe2f152-cb09-471f-ab6a-6a0167acad61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 81, 144, 1]           6,642\n",
      "            Linear-2             [-1, 144, 128]          10,496\n",
      "         LayerNorm-3             [-1, 144, 128]             256\n",
      "           Dropout-4             [-1, 145, 128]               0\n",
      "    PatchEmbedding-5             [-1, 145, 128]               0\n",
      "            Conv2d-6             [-1, 81, 1, 1]           6,642\n",
      "            Linear-7               [-1, 1, 128]          10,496\n",
      "         LayerNorm-8               [-1, 1, 128]             256\n",
      "           Dropout-9               [-1, 2, 128]               0\n",
      "   PatchEmbedding-10               [-1, 2, 128]               0\n",
      "        LayerNorm-11             [-1, 145, 128]             256\n",
      "        LayerNorm-12             [-1, 145, 128]             256\n",
      "           Linear-13             [-1, 145, 512]          65,536\n",
      "           Linear-14             [-1, 145, 512]          65,536\n",
      "           Linear-15             [-1, 145, 512]          65,536\n",
      "          Dropout-16             [-1, 145, 512]               0\n",
      "           Linear-17             [-1, 145, 128]          65,664\n",
      " HSISelfAttention-18             [-1, 145, 128]               0\n",
      "        LayerNorm-19             [-1, 145, 128]             256\n",
      "           Linear-20             [-1, 145, 256]          33,024\n",
      "             GELU-21             [-1, 145, 256]               0\n",
      "          Dropout-22             [-1, 145, 256]               0\n",
      "           Linear-23             [-1, 145, 128]          32,896\n",
      "          Dropout-24             [-1, 145, 128]               0\n",
      "              Mlp-25             [-1, 145, 128]               0\n",
      "            Block-26             [-1, 145, 128]               0\n",
      "        LayerNorm-27             [-1, 145, 128]             256\n",
      "        LayerNorm-28             [-1, 145, 128]             256\n",
      "           Linear-29             [-1, 145, 512]          65,536\n",
      "           Linear-30             [-1, 145, 512]          65,536\n",
      "           Linear-31             [-1, 145, 512]          65,536\n",
      "          Dropout-32             [-1, 145, 512]               0\n",
      "           Linear-33             [-1, 145, 128]          65,664\n",
      " HSISelfAttention-34             [-1, 145, 128]               0\n",
      "        LayerNorm-35             [-1, 145, 128]             256\n",
      "           Linear-36             [-1, 145, 256]          33,024\n",
      "             GELU-37             [-1, 145, 256]               0\n",
      "          Dropout-38             [-1, 145, 256]               0\n",
      "           Linear-39             [-1, 145, 128]          32,896\n",
      "          Dropout-40             [-1, 145, 128]               0\n",
      "              Mlp-41             [-1, 145, 128]               0\n",
      "            Block-42             [-1, 145, 128]               0\n",
      "        LayerNorm-43             [-1, 145, 128]             256\n",
      "TransformerEncoder-44             [-1, 145, 128]               0\n",
      "        LayerNorm-45               [-1, 2, 128]             256\n",
      "        LayerNorm-46               [-1, 2, 128]             256\n",
      "           Linear-47               [-1, 2, 512]          65,536\n",
      "           Linear-48               [-1, 2, 512]          65,536\n",
      "           Linear-49               [-1, 2, 512]          65,536\n",
      "          Dropout-50               [-1, 2, 512]               0\n",
      "           Linear-51               [-1, 2, 128]          65,664\n",
      " HSISelfAttention-52               [-1, 2, 128]               0\n",
      "        LayerNorm-53               [-1, 2, 128]             256\n",
      "           Linear-54               [-1, 2, 256]          33,024\n",
      "             GELU-55               [-1, 2, 256]               0\n",
      "          Dropout-56               [-1, 2, 256]               0\n",
      "           Linear-57               [-1, 2, 128]          32,896\n",
      "          Dropout-58               [-1, 2, 128]               0\n",
      "              Mlp-59               [-1, 2, 128]               0\n",
      "            Block-60               [-1, 2, 128]               0\n",
      "        LayerNorm-61               [-1, 2, 128]             256\n",
      "        LayerNorm-62               [-1, 2, 128]             256\n",
      "           Linear-63               [-1, 2, 512]          65,536\n",
      "           Linear-64               [-1, 2, 512]          65,536\n",
      "           Linear-65               [-1, 2, 512]          65,536\n",
      "          Dropout-66               [-1, 2, 512]               0\n",
      "           Linear-67               [-1, 2, 128]          65,664\n",
      " HSISelfAttention-68               [-1, 2, 128]               0\n",
      "        LayerNorm-69               [-1, 2, 128]             256\n",
      "           Linear-70               [-1, 2, 256]          33,024\n",
      "             GELU-71               [-1, 2, 256]               0\n",
      "          Dropout-72               [-1, 2, 256]               0\n",
      "           Linear-73               [-1, 2, 128]          32,896\n",
      "          Dropout-74               [-1, 2, 128]               0\n",
      "              Mlp-75               [-1, 2, 128]               0\n",
      "            Block-76               [-1, 2, 128]               0\n",
      "        LayerNorm-77               [-1, 2, 128]             256\n",
      "TransformerEncoder-78               [-1, 2, 128]               0\n",
      "        LayerNorm-79             [-1, 145, 128]             256\n",
      "        LayerNorm-80             [-1, 145, 128]             256\n",
      "           Linear-81               [-1, 2, 512]          65,536\n",
      "           Linear-82             [-1, 145, 512]          65,536\n",
      "           Linear-83             [-1, 145, 512]          65,536\n",
      "          Dropout-84               [-1, 1, 512]               0\n",
      "           Linear-85               [-1, 1, 128]          65,664\n",
      "HSICrossAttention-86               [-1, 1, 128]               0\n",
      "        LayerNorm-87             [-1, 145, 128]             256\n",
      "           Linear-88             [-1, 145, 256]          33,024\n",
      "             GELU-89             [-1, 145, 256]               0\n",
      "          Dropout-90             [-1, 145, 256]               0\n",
      "           Linear-91             [-1, 145, 128]          32,896\n",
      "          Dropout-92             [-1, 145, 128]               0\n",
      "              Mlp-93             [-1, 145, 128]               0\n",
      "           BlockC-94             [-1, 145, 128]               0\n",
      "        LayerNorm-95             [-1, 145, 128]             256\n",
      "TransformerEncoderC-96             [-1, 145, 128]               0\n",
      "        LayerNorm-97               [-1, 2, 128]             256\n",
      "        LayerNorm-98               [-1, 2, 128]             256\n",
      "           Linear-99             [-1, 145, 512]          65,536\n",
      "          Linear-100               [-1, 2, 512]          65,536\n",
      "          Linear-101               [-1, 2, 512]          65,536\n",
      "         Dropout-102               [-1, 1, 512]               0\n",
      "          Linear-103               [-1, 1, 128]          65,664\n",
      "HSICrossAttention-104               [-1, 1, 128]               0\n",
      "       LayerNorm-105               [-1, 2, 128]             256\n",
      "          Linear-106               [-1, 2, 256]          33,024\n",
      "            GELU-107               [-1, 2, 256]               0\n",
      "         Dropout-108               [-1, 2, 256]               0\n",
      "          Linear-109               [-1, 2, 128]          32,896\n",
      "         Dropout-110               [-1, 2, 128]               0\n",
      "             Mlp-111               [-1, 2, 128]               0\n",
      "          BlockC-112               [-1, 2, 128]               0\n",
      "       LayerNorm-113               [-1, 2, 128]             256\n",
      "TransformerEncoderC-114               [-1, 2, 128]               0\n",
      "          Linear-115                   [-1, 15]           1,935\n",
      "          Linear-116                   [-1, 15]           1,935\n",
      "================================================================\n",
      "Total params: 2,013,442\n",
      "Trainable params: 2,013,442\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.60\n",
      "Forward/backward pass size (MB): 13.74\n",
      "Params size (MB): 7.68\n",
      "Estimated Total Size (MB): 25.02\n",
      "----------------------------------------------------------------\n",
      "Epoch [1/200], Training Loss: 2.7120, Training Accuracy: 16.45%, Test Loss: 2.6481, Test Accuracy: 11.80%\n",
      "\n",
      "\n",
      "Test accuracy increased (-inf --> 11.7980). Saving model ...\n",
      "Epoch [2/200], Training Loss: 2.3819, Training Accuracy: 27.05%, Test Loss: 2.3199, Test Accuracy: 19.67%\n",
      "\n",
      "\n",
      "Test accuracy increased (11.7980 --> 19.6688). Saving model ...\n",
      "Epoch [3/200], Training Loss: 2.0692, Training Accuracy: 35.35%, Test Loss: 2.0625, Test Accuracy: 29.68%\n",
      "\n",
      "\n",
      "Test accuracy increased (19.6688 --> 29.6794). Saving model ...\n",
      "Epoch [4/200], Training Loss: 1.7731, Training Accuracy: 51.06%, Test Loss: 1.7526, Test Accuracy: 41.82%\n",
      "\n",
      "\n",
      "Test accuracy increased (29.6794 --> 41.8218). Saving model ...\n",
      "Epoch [5/200], Training Loss: 1.4852, Training Accuracy: 58.83%, Test Loss: 1.5918, Test Accuracy: 47.24%\n",
      "\n",
      "\n",
      "Test accuracy increased (41.8218 --> 47.2411). Saving model ...\n",
      "Epoch [6/200], Training Loss: 1.2881, Training Accuracy: 62.68%, Test Loss: 1.5039, Test Accuracy: 52.22%\n",
      "\n",
      "\n",
      "Test accuracy increased (47.2411 --> 52.2178). Saving model ...\n",
      "Epoch [7/200], Training Loss: 1.1841, Training Accuracy: 65.71%, Test Loss: 1.4075, Test Accuracy: 50.64%\n",
      "Epoch [8/200], Training Loss: 0.9682, Training Accuracy: 70.73%, Test Loss: 1.3102, Test Accuracy: 56.20%\n",
      "\n",
      "\n",
      "Test accuracy increased (52.2178 --> 56.2023). Saving model ...\n",
      "Epoch [9/200], Training Loss: 0.8792, Training Accuracy: 74.82%, Test Loss: 1.2186, Test Accuracy: 59.86%\n",
      "\n",
      "\n",
      "Test accuracy increased (56.2023 --> 59.8590). Saving model ...\n",
      "Epoch [10/200], Training Loss: 0.7804, Training Accuracy: 77.79%, Test Loss: 1.1824, Test Accuracy: 61.65%\n",
      "\n",
      "\n",
      "Test accuracy increased (59.8590 --> 61.6463). Saving model ...\n",
      "Epoch [11/200], Training Loss: 0.6847, Training Accuracy: 80.83%, Test Loss: 1.0353, Test Accuracy: 66.39%\n",
      "\n",
      "\n",
      "Test accuracy increased (61.6463 --> 66.3934). Saving model ...\n",
      "Epoch [12/200], Training Loss: 0.5862, Training Accuracy: 81.39%, Test Loss: 0.9984, Test Accuracy: 65.60%\n",
      "Epoch [13/200], Training Loss: 0.5324, Training Accuracy: 83.44%, Test Loss: 0.9178, Test Accuracy: 68.35%\n",
      "\n",
      "\n",
      "Test accuracy increased (66.3934 --> 68.3529). Saving model ...\n",
      "Epoch [14/200], Training Loss: 0.4506, Training Accuracy: 89.02%, Test Loss: 0.8684, Test Accuracy: 71.32%\n",
      "\n",
      "\n",
      "Test accuracy increased (68.3529 --> 71.3208). Saving model ...\n",
      "Epoch [15/200], Training Loss: 0.3994, Training Accuracy: 88.88%, Test Loss: 0.8845, Test Accuracy: 70.77%\n",
      "Epoch [16/200], Training Loss: 0.3424, Training Accuracy: 88.81%, Test Loss: 0.8931, Test Accuracy: 71.59%\n",
      "\n",
      "\n",
      "Test accuracy increased (71.3208 --> 71.5914). Saving model ...\n",
      "Epoch [17/200], Training Loss: 0.3333, Training Accuracy: 90.54%, Test Loss: 0.8487, Test Accuracy: 72.86%\n",
      "\n",
      "\n",
      "Test accuracy increased (71.5914 --> 72.8622). Saving model ...\n",
      "Epoch [18/200], Training Loss: 0.3414, Training Accuracy: 92.55%, Test Loss: 0.7837, Test Accuracy: 75.10%\n",
      "\n",
      "\n",
      "Test accuracy increased (72.8622 --> 75.1004). Saving model ...\n",
      "Epoch [19/200], Training Loss: 0.3079, Training Accuracy: 92.90%, Test Loss: 0.7962, Test Accuracy: 74.51%\n",
      "Epoch [20/200], Training Loss: 0.2708, Training Accuracy: 95.44%, Test Loss: 0.7103, Test Accuracy: 77.14%\n",
      "\n",
      "\n",
      "Test accuracy increased (75.1004 --> 77.1419). Saving model ...\n",
      "Epoch [21/200], Training Loss: 0.2194, Training Accuracy: 95.23%, Test Loss: 0.7184, Test Accuracy: 77.02%\n",
      "Epoch [22/200], Training Loss: 0.2372, Training Accuracy: 93.57%, Test Loss: 0.8497, Test Accuracy: 73.59%\n",
      "Epoch [23/200], Training Loss: 0.1971, Training Accuracy: 96.68%, Test Loss: 0.7247, Test Accuracy: 78.17%\n",
      "\n",
      "\n",
      "Test accuracy increased (77.1419 --> 78.1668). Saving model ...\n",
      "Epoch [24/200], Training Loss: 0.2302, Training Accuracy: 94.63%, Test Loss: 0.7476, Test Accuracy: 77.40%\n",
      "Epoch [25/200], Training Loss: 0.1455, Training Accuracy: 96.43%, Test Loss: 0.7169, Test Accuracy: 77.98%\n",
      "Epoch [26/200], Training Loss: 0.1688, Training Accuracy: 93.29%, Test Loss: 0.8163, Test Accuracy: 76.04%\n",
      "Epoch [27/200], Training Loss: 0.2126, Training Accuracy: 96.19%, Test Loss: 0.7775, Test Accuracy: 76.67%\n",
      "Epoch [28/200], Training Loss: 0.1462, Training Accuracy: 95.37%, Test Loss: 0.8193, Test Accuracy: 76.10%\n",
      "Epoch [29/200], Training Loss: 0.1196, Training Accuracy: 92.58%, Test Loss: 0.8177, Test Accuracy: 75.12%\n",
      "Epoch [30/200], Training Loss: 0.1654, Training Accuracy: 96.08%, Test Loss: 0.7445, Test Accuracy: 77.92%\n",
      "Epoch [31/200], Training Loss: 0.1155, Training Accuracy: 97.21%, Test Loss: 0.7508, Test Accuracy: 78.69%\n",
      "\n",
      "\n",
      "Test accuracy increased (78.1668 --> 78.6915). Saving model ...\n",
      "Epoch [32/200], Training Loss: 0.1019, Training Accuracy: 98.73%, Test Loss: 0.6591, Test Accuracy: 80.97%\n",
      "\n",
      "\n",
      "Test accuracy increased (78.6915 --> 80.9707). Saving model ...\n",
      "Epoch [33/200], Training Loss: 0.0894, Training Accuracy: 97.00%, Test Loss: 0.7825, Test Accuracy: 77.28%\n",
      "Epoch [34/200], Training Loss: 0.1028, Training Accuracy: 95.76%, Test Loss: 0.8490, Test Accuracy: 77.22%\n",
      "Epoch [35/200], Training Loss: 0.1188, Training Accuracy: 98.06%, Test Loss: 0.6983, Test Accuracy: 80.04%\n",
      "Epoch [36/200], Training Loss: 0.1275, Training Accuracy: 91.21%, Test Loss: 1.0936, Test Accuracy: 72.04%\n",
      "Epoch [37/200], Training Loss: 0.1390, Training Accuracy: 97.10%, Test Loss: 0.7747, Test Accuracy: 78.33%\n",
      "Epoch [38/200], Training Loss: 0.1367, Training Accuracy: 96.12%, Test Loss: 0.7845, Test Accuracy: 78.49%\n",
      "Epoch [39/200], Training Loss: 0.1121, Training Accuracy: 97.60%, Test Loss: 0.7307, Test Accuracy: 79.84%\n",
      "Epoch [40/200], Training Loss: 0.0843, Training Accuracy: 99.26%, Test Loss: 0.6836, Test Accuracy: 81.29%\n",
      "\n",
      "\n",
      "Test accuracy increased (80.9707 --> 81.2905). Saving model ...\n",
      "Epoch [41/200], Training Loss: 0.0643, Training Accuracy: 99.51%, Test Loss: 0.6434, Test Accuracy: 82.60%\n",
      "\n",
      "\n",
      "Test accuracy increased (81.2905 --> 82.6023). Saving model ...\n",
      "Epoch [42/200], Training Loss: 0.0436, Training Accuracy: 99.72%, Test Loss: 0.6067, Test Accuracy: 83.77%\n",
      "\n",
      "\n",
      "Test accuracy increased (82.6023 --> 83.7665). Saving model ...\n",
      "Epoch [43/200], Training Loss: 0.0393, Training Accuracy: 99.51%, Test Loss: 0.6069, Test Accuracy: 83.59%\n",
      "Epoch [44/200], Training Loss: 0.0996, Training Accuracy: 92.09%, Test Loss: 0.9463, Test Accuracy: 74.57%\n",
      "Epoch [45/200], Training Loss: 0.0889, Training Accuracy: 95.20%, Test Loss: 0.8248, Test Accuracy: 77.76%\n",
      "Epoch [46/200], Training Loss: 0.0795, Training Accuracy: 99.19%, Test Loss: 0.6854, Test Accuracy: 82.11%\n",
      "Epoch [47/200], Training Loss: 0.0743, Training Accuracy: 96.57%, Test Loss: 0.8795, Test Accuracy: 77.43%\n",
      "Epoch [48/200], Training Loss: 0.1349, Training Accuracy: 93.75%, Test Loss: 0.9500, Test Accuracy: 75.49%\n",
      "Epoch [49/200], Training Loss: 0.1484, Training Accuracy: 97.49%, Test Loss: 0.7516, Test Accuracy: 79.01%\n",
      "Epoch [50/200], Training Loss: 0.1243, Training Accuracy: 95.87%, Test Loss: 0.9057, Test Accuracy: 76.92%\n",
      "Epoch [51/200], Training Loss: 0.1048, Training Accuracy: 96.65%, Test Loss: 0.8624, Test Accuracy: 77.83%\n",
      "Epoch [52/200], Training Loss: 0.1186, Training Accuracy: 97.78%, Test Loss: 0.8452, Test Accuracy: 78.98%\n",
      "Epoch [53/200], Training Loss: 0.0807, Training Accuracy: 99.26%, Test Loss: 0.7022, Test Accuracy: 81.98%\n",
      "Epoch [54/200], Training Loss: 0.0584, Training Accuracy: 99.72%, Test Loss: 0.6819, Test Accuracy: 82.13%\n",
      "Epoch [55/200], Training Loss: 0.0344, Training Accuracy: 99.54%, Test Loss: 0.6782, Test Accuracy: 82.77%\n",
      "Epoch [56/200], Training Loss: 0.0294, Training Accuracy: 99.82%, Test Loss: 0.6943, Test Accuracy: 82.55%\n",
      "Epoch [57/200], Training Loss: 0.0206, Training Accuracy: 99.86%, Test Loss: 0.6458, Test Accuracy: 84.03%\n",
      "\n",
      "\n",
      "Test accuracy increased (83.7665 --> 84.0289). Saving model ...\n",
      "Epoch [58/200], Training Loss: 0.0072, Training Accuracy: 99.93%, Test Loss: 0.6422, Test Accuracy: 84.19%\n",
      "\n",
      "\n",
      "Test accuracy increased (84.0289 --> 84.1928). Saving model ...\n",
      "Epoch [59/200], Training Loss: 0.0329, Training Accuracy: 99.61%, Test Loss: 0.7250, Test Accuracy: 81.99%\n",
      "Epoch [60/200], Training Loss: 0.0812, Training Accuracy: 88.28%, Test Loss: 1.2565, Test Accuracy: 69.10%\n",
      "Epoch [61/200], Training Loss: 0.2869, Training Accuracy: 87.04%, Test Loss: 1.0284, Test Accuracy: 71.71%\n",
      "Epoch [62/200], Training Loss: 0.2571, Training Accuracy: 98.69%, Test Loss: 0.7112, Test Accuracy: 80.99%\n",
      "Epoch [63/200], Training Loss: 0.0989, Training Accuracy: 97.78%, Test Loss: 0.8315, Test Accuracy: 79.04%\n",
      "Epoch [64/200], Training Loss: 0.1266, Training Accuracy: 98.45%, Test Loss: 0.8525, Test Accuracy: 78.72%\n",
      "Epoch [65/200], Training Loss: 0.0437, Training Accuracy: 99.68%, Test Loss: 0.7663, Test Accuracy: 80.75%\n",
      "Epoch [66/200], Training Loss: 0.0656, Training Accuracy: 99.79%, Test Loss: 0.6874, Test Accuracy: 82.28%\n",
      "Epoch [67/200], Training Loss: 0.0926, Training Accuracy: 98.76%, Test Loss: 0.7036, Test Accuracy: 81.63%\n",
      "Epoch [68/200], Training Loss: 0.0395, Training Accuracy: 99.65%, Test Loss: 0.6324, Test Accuracy: 83.63%\n",
      "Epoch [69/200], Training Loss: 0.0168, Training Accuracy: 100.00%, Test Loss: 0.6003, Test Accuracy: 85.07%\n",
      "\n",
      "\n",
      "Test accuracy increased (84.1928 --> 85.0701). Saving model ...\n",
      "Epoch [70/200], Training Loss: 0.0198, Training Accuracy: 99.82%, Test Loss: 0.6175, Test Accuracy: 84.34%\n",
      "Epoch [71/200], Training Loss: 0.0310, Training Accuracy: 99.44%, Test Loss: 0.6669, Test Accuracy: 83.68%\n",
      "Epoch [72/200], Training Loss: 0.0520, Training Accuracy: 99.86%, Test Loss: 0.6173, Test Accuracy: 84.54%\n",
      "Epoch [73/200], Training Loss: 0.0225, Training Accuracy: 99.82%, Test Loss: 0.6474, Test Accuracy: 83.96%\n",
      "Epoch [74/200], Training Loss: 0.0290, Training Accuracy: 99.72%, Test Loss: 0.6306, Test Accuracy: 84.69%\n",
      "Epoch [75/200], Training Loss: 0.0065, Training Accuracy: 100.00%, Test Loss: 0.6023, Test Accuracy: 85.49%\n",
      "\n",
      "\n",
      "Test accuracy increased (85.0701 --> 85.4882). Saving model ...\n",
      "Epoch [76/200], Training Loss: 0.0103, Training Accuracy: 99.29%, Test Loss: 0.7732, Test Accuracy: 82.18%\n",
      "Epoch [77/200], Training Loss: 0.0422, Training Accuracy: 99.79%, Test Loss: 0.6958, Test Accuracy: 83.37%\n",
      "Epoch [78/200], Training Loss: 0.0630, Training Accuracy: 96.01%, Test Loss: 1.0428, Test Accuracy: 76.37%\n",
      "Epoch [79/200], Training Loss: 0.0828, Training Accuracy: 99.58%, Test Loss: 0.7238, Test Accuracy: 82.41%\n",
      "Epoch [80/200], Training Loss: 0.1150, Training Accuracy: 97.63%, Test Loss: 0.8340, Test Accuracy: 80.08%\n",
      "Epoch [81/200], Training Loss: 0.1678, Training Accuracy: 95.66%, Test Loss: 0.9338, Test Accuracy: 76.13%\n",
      "Epoch [82/200], Training Loss: 0.1181, Training Accuracy: 98.55%, Test Loss: 0.7200, Test Accuracy: 81.35%\n",
      "Epoch [83/200], Training Loss: 0.0556, Training Accuracy: 99.36%, Test Loss: 0.7034, Test Accuracy: 82.73%\n",
      "Epoch [84/200], Training Loss: 0.0251, Training Accuracy: 99.96%, Test Loss: 0.6263, Test Accuracy: 84.88%\n",
      "Epoch [85/200], Training Loss: 0.0123, Training Accuracy: 100.00%, Test Loss: 0.5832, Test Accuracy: 85.68%\n",
      "\n",
      "\n",
      "Test accuracy increased (85.4882 --> 85.6768). Saving model ...\n",
      "Epoch [86/200], Training Loss: 0.0101, Training Accuracy: 99.72%, Test Loss: 0.7588, Test Accuracy: 82.12%\n",
      "Epoch [87/200], Training Loss: 0.0137, Training Accuracy: 99.68%, Test Loss: 0.6827, Test Accuracy: 83.33%\n",
      "Epoch [88/200], Training Loss: 0.0244, Training Accuracy: 99.61%, Test Loss: 0.7027, Test Accuracy: 83.14%\n",
      "Epoch [89/200], Training Loss: 0.0340, Training Accuracy: 99.08%, Test Loss: 0.7123, Test Accuracy: 82.54%\n",
      "Epoch [90/200], Training Loss: 0.0308, Training Accuracy: 99.79%, Test Loss: 0.7089, Test Accuracy: 83.53%\n",
      "Epoch [91/200], Training Loss: 0.0287, Training Accuracy: 99.47%, Test Loss: 0.7468, Test Accuracy: 82.46%\n",
      "Epoch [92/200], Training Loss: 0.0225, Training Accuracy: 99.89%, Test Loss: 0.6408, Test Accuracy: 84.67%\n",
      "Epoch [93/200], Training Loss: 0.0062, Training Accuracy: 99.93%, Test Loss: 0.6330, Test Accuracy: 85.34%\n",
      "Epoch [94/200], Training Loss: 0.0034, Training Accuracy: 99.93%, Test Loss: 0.6526, Test Accuracy: 84.93%\n",
      "Epoch [95/200], Training Loss: 0.0083, Training Accuracy: 100.00%, Test Loss: 0.6477, Test Accuracy: 85.23%\n",
      "Epoch [96/200], Training Loss: 0.0065, Training Accuracy: 99.89%, Test Loss: 0.6732, Test Accuracy: 84.72%\n",
      "Epoch [97/200], Training Loss: 0.0226, Training Accuracy: 99.72%, Test Loss: 0.7342, Test Accuracy: 83.16%\n",
      "Epoch [98/200], Training Loss: 0.0406, Training Accuracy: 98.13%, Test Loss: 0.9080, Test Accuracy: 79.86%\n",
      "Epoch [99/200], Training Loss: 0.1819, Training Accuracy: 96.26%, Test Loss: 0.9405, Test Accuracy: 77.64%\n",
      "Epoch [100/200], Training Loss: 0.1961, Training Accuracy: 96.57%, Test Loss: 0.9137, Test Accuracy: 77.26%\n",
      "Epoch [101/200], Training Loss: 0.1309, Training Accuracy: 94.63%, Test Loss: 0.9871, Test Accuracy: 75.67%\n",
      "Epoch [102/200], Training Loss: 0.0852, Training Accuracy: 98.59%, Test Loss: 0.7639, Test Accuracy: 80.45%\n",
      "Epoch [103/200], Training Loss: 0.0453, Training Accuracy: 99.12%, Test Loss: 0.7578, Test Accuracy: 81.32%\n",
      "Epoch [104/200], Training Loss: 0.0553, Training Accuracy: 98.94%, Test Loss: 0.8101, Test Accuracy: 80.98%\n",
      "Epoch [105/200], Training Loss: 0.0993, Training Accuracy: 99.05%, Test Loss: 0.7289, Test Accuracy: 82.01%\n",
      "Epoch [106/200], Training Loss: 0.0764, Training Accuracy: 97.28%, Test Loss: 0.8478, Test Accuracy: 79.22%\n",
      "Epoch [107/200], Training Loss: 0.0909, Training Accuracy: 99.19%, Test Loss: 0.7759, Test Accuracy: 81.16%\n",
      "Epoch [108/200], Training Loss: 0.0850, Training Accuracy: 98.38%, Test Loss: 0.7665, Test Accuracy: 80.35%\n",
      "Epoch [109/200], Training Loss: 0.0968, Training Accuracy: 99.19%, Test Loss: 0.7203, Test Accuracy: 81.56%\n",
      "Epoch [110/200], Training Loss: 0.0468, Training Accuracy: 99.51%, Test Loss: 0.6543, Test Accuracy: 84.00%\n",
      "Epoch [111/200], Training Loss: 0.0610, Training Accuracy: 99.51%, Test Loss: 0.7569, Test Accuracy: 82.58%\n",
      "Epoch [112/200], Training Loss: 0.0415, Training Accuracy: 99.26%, Test Loss: 0.7633, Test Accuracy: 81.68%\n",
      "Epoch [113/200], Training Loss: 0.0431, Training Accuracy: 99.72%, Test Loss: 0.7371, Test Accuracy: 82.80%\n",
      "Epoch [114/200], Training Loss: 0.0132, Training Accuracy: 99.93%, Test Loss: 0.6588, Test Accuracy: 84.68%\n",
      "Epoch [115/200], Training Loss: 0.0145, Training Accuracy: 99.82%, Test Loss: 0.7629, Test Accuracy: 83.03%\n",
      "Epoch [116/200], Training Loss: 0.0097, Training Accuracy: 99.93%, Test Loss: 0.6614, Test Accuracy: 85.14%\n",
      "Epoch [117/200], Training Loss: 0.0083, Training Accuracy: 99.89%, Test Loss: 0.6461, Test Accuracy: 85.35%\n",
      "Epoch [118/200], Training Loss: 0.0036, Training Accuracy: 100.00%, Test Loss: 0.6029, Test Accuracy: 86.13%\n",
      "\n",
      "\n",
      "Test accuracy increased (85.6768 --> 86.1277). Saving model ...\n",
      "Epoch [119/200], Training Loss: 0.0028, Training Accuracy: 100.00%, Test Loss: 0.6567, Test Accuracy: 85.53%\n",
      "Epoch [120/200], Training Loss: 0.0019, Training Accuracy: 100.00%, Test Loss: 0.6148, Test Accuracy: 86.28%\n",
      "\n",
      "\n",
      "Test accuracy increased (86.1277 --> 86.2753). Saving model ...\n",
      "Epoch [121/200], Training Loss: 0.0018, Training Accuracy: 100.00%, Test Loss: 0.6035, Test Accuracy: 86.34%\n",
      "\n",
      "\n",
      "Test accuracy increased (86.2753 --> 86.3409). Saving model ...\n",
      "Epoch [122/200], Training Loss: 0.0013, Training Accuracy: 100.00%, Test Loss: 0.6109, Test Accuracy: 86.06%\n",
      "Epoch [123/200], Training Loss: 0.0010, Training Accuracy: 100.00%, Test Loss: 0.5978, Test Accuracy: 86.51%\n",
      "\n",
      "\n",
      "Test accuracy increased (86.3409 --> 86.5131). Saving model ...\n",
      "Epoch [124/200], Training Loss: 0.0007, Training Accuracy: 100.00%, Test Loss: 0.6046, Test Accuracy: 86.41%\n",
      "Epoch [125/200], Training Loss: 0.0008, Training Accuracy: 100.00%, Test Loss: 0.6001, Test Accuracy: 86.57%\n",
      "\n",
      "\n",
      "Test accuracy increased (86.5131 --> 86.5705). Saving model ...\n",
      "Epoch [126/200], Training Loss: 0.0005, Training Accuracy: 100.00%, Test Loss: 0.6017, Test Accuracy: 86.43%\n",
      "Epoch [127/200], Training Loss: 0.0006, Training Accuracy: 100.00%, Test Loss: 0.6027, Test Accuracy: 86.61%\n",
      "\n",
      "\n",
      "Test accuracy increased (86.5705 --> 86.6115). Saving model ...\n",
      "Epoch [128/200], Training Loss: 0.0006, Training Accuracy: 100.00%, Test Loss: 0.6030, Test Accuracy: 86.78%\n",
      "\n",
      "\n",
      "Test accuracy increased (86.6115 --> 86.7754). Saving model ...\n",
      "Epoch [129/200], Training Loss: 0.0005, Training Accuracy: 100.00%, Test Loss: 0.6008, Test Accuracy: 86.86%\n",
      "\n",
      "\n",
      "Test accuracy increased (86.7754 --> 86.8574). Saving model ...\n",
      "Epoch [130/200], Training Loss: 0.0004, Training Accuracy: 100.00%, Test Loss: 0.6048, Test Accuracy: 86.75%\n",
      "Epoch [131/200], Training Loss: 0.0004, Training Accuracy: 100.00%, Test Loss: 0.6063, Test Accuracy: 86.76%\n",
      "Epoch [132/200], Training Loss: 0.0004, Training Accuracy: 100.00%, Test Loss: 0.6098, Test Accuracy: 86.73%\n",
      "Epoch [133/200], Training Loss: 0.0004, Training Accuracy: 100.00%, Test Loss: 0.6083, Test Accuracy: 86.82%\n",
      "Epoch [134/200], Training Loss: 0.0004, Training Accuracy: 99.96%, Test Loss: 0.6099, Test Accuracy: 86.85%\n",
      "Epoch [135/200], Training Loss: 0.0004, Training Accuracy: 100.00%, Test Loss: 0.6039, Test Accuracy: 87.03%\n",
      "\n",
      "\n",
      "Test accuracy increased (86.8574 --> 87.0296). Saving model ...\n",
      "Epoch [136/200], Training Loss: 0.0004, Training Accuracy: 100.00%, Test Loss: 0.6052, Test Accuracy: 86.98%\n",
      "Epoch [137/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.6093, Test Accuracy: 86.93%\n",
      "Epoch [138/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.6077, Test Accuracy: 86.94%\n",
      "Epoch [139/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.6112, Test Accuracy: 86.93%\n",
      "Epoch [140/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.6091, Test Accuracy: 87.08%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.0296 --> 87.0788). Saving model ...\n",
      "Epoch [141/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.6113, Test Accuracy: 87.09%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.0788 --> 87.0870). Saving model ...\n",
      "Epoch [142/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.6225, Test Accuracy: 86.84%\n",
      "Epoch [143/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.6194, Test Accuracy: 86.87%\n",
      "Epoch [144/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6218, Test Accuracy: 86.82%\n",
      "Epoch [145/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6226, Test Accuracy: 86.89%\n",
      "Epoch [146/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6216, Test Accuracy: 86.86%\n",
      "Epoch [147/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6271, Test Accuracy: 86.82%\n",
      "Epoch [148/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6245, Test Accuracy: 86.94%\n",
      "Epoch [149/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6234, Test Accuracy: 86.95%\n",
      "Epoch [150/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6243, Test Accuracy: 86.98%\n",
      "Epoch [151/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6271, Test Accuracy: 86.89%\n",
      "Epoch [152/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6286, Test Accuracy: 86.91%\n",
      "Epoch [153/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6289, Test Accuracy: 86.88%\n",
      "Epoch [154/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6250, Test Accuracy: 86.96%\n",
      "Epoch [155/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6271, Test Accuracy: 87.03%\n",
      "Epoch [156/200], Training Loss: 0.0004, Training Accuracy: 100.00%, Test Loss: 0.6356, Test Accuracy: 86.92%\n",
      "Epoch [157/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6328, Test Accuracy: 87.10%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.0870 --> 87.1034). Saving model ...\n",
      "Epoch [158/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6379, Test Accuracy: 86.93%\n",
      "Epoch [159/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6375, Test Accuracy: 86.96%\n",
      "Epoch [160/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6376, Test Accuracy: 87.01%\n",
      "Epoch [161/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6237, Test Accuracy: 87.34%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.1034 --> 87.3411). Saving model ...\n",
      "Epoch [162/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6156, Test Accuracy: 87.48%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.3411 --> 87.4805). Saving model ...\n",
      "Epoch [163/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6291, Test Accuracy: 87.40%\n",
      "Epoch [164/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6233, Test Accuracy: 87.49%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.4805 --> 87.4887). Saving model ...\n",
      "Epoch [165/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6294, Test Accuracy: 87.26%\n",
      "Epoch [166/200], Training Loss: 0.0001, Training Accuracy: 100.00%, Test Loss: 0.6350, Test Accuracy: 87.18%\n",
      "Epoch [167/200], Training Loss: 0.0002, Training Accuracy: 99.93%, Test Loss: 0.6585, Test Accuracy: 86.80%\n",
      "Epoch [168/200], Training Loss: 0.0007, Training Accuracy: 100.00%, Test Loss: 0.7868, Test Accuracy: 84.41%\n",
      "Epoch [169/200], Training Loss: 0.0006, Training Accuracy: 99.93%, Test Loss: 0.7421, Test Accuracy: 85.34%\n",
      "Epoch [170/200], Training Loss: 0.6197, Training Accuracy: 67.20%, Test Loss: 1.6588, Test Accuracy: 54.21%\n",
      "Epoch [171/200], Training Loss: 0.6388, Training Accuracy: 90.40%, Test Loss: 0.8780, Test Accuracy: 74.59%\n",
      "Epoch [172/200], Training Loss: 0.2794, Training Accuracy: 93.11%, Test Loss: 0.8889, Test Accuracy: 75.14%\n",
      "Epoch [173/200], Training Loss: 0.2678, Training Accuracy: 95.69%, Test Loss: 0.7289, Test Accuracy: 79.63%\n",
      "Epoch [174/200], Training Loss: 0.1389, Training Accuracy: 96.79%, Test Loss: 0.7309, Test Accuracy: 80.50%\n",
      "Epoch [175/200], Training Loss: 0.1121, Training Accuracy: 98.45%, Test Loss: 0.6651, Test Accuracy: 82.54%\n",
      "Epoch [176/200], Training Loss: 0.1011, Training Accuracy: 98.34%, Test Loss: 0.7226, Test Accuracy: 80.46%\n",
      "Epoch [177/200], Training Loss: 0.0628, Training Accuracy: 99.29%, Test Loss: 0.6459, Test Accuracy: 83.03%\n",
      "Epoch [178/200], Training Loss: 0.0566, Training Accuracy: 99.61%, Test Loss: 0.6000, Test Accuracy: 84.13%\n",
      "Epoch [179/200], Training Loss: 0.0315, Training Accuracy: 99.54%, Test Loss: 0.6695, Test Accuracy: 82.91%\n",
      "Epoch [180/200], Training Loss: 0.0305, Training Accuracy: 99.68%, Test Loss: 0.6224, Test Accuracy: 84.05%\n",
      "Epoch [181/200], Training Loss: 0.0333, Training Accuracy: 99.79%, Test Loss: 0.6808, Test Accuracy: 83.32%\n",
      "Epoch [182/200], Training Loss: 0.0517, Training Accuracy: 99.58%, Test Loss: 0.6590, Test Accuracy: 83.62%\n",
      "Epoch [183/200], Training Loss: 0.0583, Training Accuracy: 97.99%, Test Loss: 0.8150, Test Accuracy: 79.82%\n",
      "Epoch [184/200], Training Loss: 0.0973, Training Accuracy: 97.03%, Test Loss: 0.9035, Test Accuracy: 77.54%\n",
      "Epoch [185/200], Training Loss: 0.0535, Training Accuracy: 99.58%, Test Loss: 0.6428, Test Accuracy: 84.08%\n",
      "Epoch [186/200], Training Loss: 0.0322, Training Accuracy: 99.58%, Test Loss: 0.6559, Test Accuracy: 84.36%\n",
      "Epoch [187/200], Training Loss: 0.0146, Training Accuracy: 99.72%, Test Loss: 0.6484, Test Accuracy: 84.68%\n",
      "Epoch [188/200], Training Loss: 0.0171, Training Accuracy: 100.00%, Test Loss: 0.6220, Test Accuracy: 85.63%\n",
      "Epoch [189/200], Training Loss: 0.0084, Training Accuracy: 99.72%, Test Loss: 0.6657, Test Accuracy: 84.77%\n",
      "Epoch [190/200], Training Loss: 0.0080, Training Accuracy: 100.00%, Test Loss: 0.6069, Test Accuracy: 86.07%\n",
      "Epoch [191/200], Training Loss: 0.0025, Training Accuracy: 100.00%, Test Loss: 0.5890, Test Accuracy: 86.51%\n",
      "Epoch [192/200], Training Loss: 0.0029, Training Accuracy: 100.00%, Test Loss: 0.6166, Test Accuracy: 85.87%\n",
      "Epoch [193/200], Training Loss: 0.0087, Training Accuracy: 99.96%, Test Loss: 0.5861, Test Accuracy: 86.88%\n",
      "Epoch [194/200], Training Loss: 0.0107, Training Accuracy: 99.05%, Test Loss: 0.6617, Test Accuracy: 83.84%\n",
      "Epoch [195/200], Training Loss: 0.0284, Training Accuracy: 99.75%, Test Loss: 0.6874, Test Accuracy: 84.11%\n",
      "Epoch [196/200], Training Loss: 0.0306, Training Accuracy: 99.08%, Test Loss: 0.7147, Test Accuracy: 83.58%\n",
      "Epoch [197/200], Training Loss: 0.0635, Training Accuracy: 98.73%, Test Loss: 0.7111, Test Accuracy: 83.06%\n",
      "Epoch [198/200], Training Loss: 0.0888, Training Accuracy: 97.85%, Test Loss: 0.8648, Test Accuracy: 80.53%\n",
      "Epoch [199/200], Training Loss: 0.0607, Training Accuracy: 98.91%, Test Loss: 0.6844, Test Accuracy: 83.07%\n",
      "Epoch [200/200], Training Loss: 0.0985, Training Accuracy: 95.48%, Test Loss: 0.9193, Test Accuracy: 77.58%\n",
      "\n",
      "\n",
      "\n",
      "The train time (in seconds) is: 454.37442088127136\n",
      "Test Loss: 0.6233\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 81, 144, 1]           6,642\n",
      "            Linear-2             [-1, 144, 128]          10,496\n",
      "         LayerNorm-3             [-1, 144, 128]             256\n",
      "           Dropout-4             [-1, 145, 128]               0\n",
      "    PatchEmbedding-5             [-1, 145, 128]               0\n",
      "            Conv2d-6             [-1, 81, 1, 1]           6,642\n",
      "            Linear-7               [-1, 1, 128]          10,496\n",
      "         LayerNorm-8               [-1, 1, 128]             256\n",
      "           Dropout-9               [-1, 2, 128]               0\n",
      "   PatchEmbedding-10               [-1, 2, 128]               0\n",
      "        LayerNorm-11             [-1, 145, 128]             256\n",
      "        LayerNorm-12             [-1, 145, 128]             256\n",
      "           Linear-13             [-1, 145, 512]          65,536\n",
      "           Linear-14             [-1, 145, 512]          65,536\n",
      "           Linear-15             [-1, 145, 512]          65,536\n",
      "          Dropout-16             [-1, 145, 512]               0\n",
      "           Linear-17             [-1, 145, 128]          65,664\n",
      " HSISelfAttention-18             [-1, 145, 128]               0\n",
      "        LayerNorm-19             [-1, 145, 128]             256\n",
      "           Linear-20             [-1, 145, 256]          33,024\n",
      "             GELU-21             [-1, 145, 256]               0\n",
      "          Dropout-22             [-1, 145, 256]               0\n",
      "           Linear-23             [-1, 145, 128]          32,896\n",
      "          Dropout-24             [-1, 145, 128]               0\n",
      "              Mlp-25             [-1, 145, 128]               0\n",
      "            Block-26             [-1, 145, 128]               0\n",
      "        LayerNorm-27             [-1, 145, 128]             256\n",
      "        LayerNorm-28             [-1, 145, 128]             256\n",
      "           Linear-29             [-1, 145, 512]          65,536\n",
      "           Linear-30             [-1, 145, 512]          65,536\n",
      "           Linear-31             [-1, 145, 512]          65,536\n",
      "          Dropout-32             [-1, 145, 512]               0\n",
      "           Linear-33             [-1, 145, 128]          65,664\n",
      " HSISelfAttention-34             [-1, 145, 128]               0\n",
      "        LayerNorm-35             [-1, 145, 128]             256\n",
      "           Linear-36             [-1, 145, 256]          33,024\n",
      "             GELU-37             [-1, 145, 256]               0\n",
      "          Dropout-38             [-1, 145, 256]               0\n",
      "           Linear-39             [-1, 145, 128]          32,896\n",
      "          Dropout-40             [-1, 145, 128]               0\n",
      "              Mlp-41             [-1, 145, 128]               0\n",
      "            Block-42             [-1, 145, 128]               0\n",
      "        LayerNorm-43             [-1, 145, 128]             256\n",
      "TransformerEncoder-44             [-1, 145, 128]               0\n",
      "        LayerNorm-45               [-1, 2, 128]             256\n",
      "        LayerNorm-46               [-1, 2, 128]             256\n",
      "           Linear-47               [-1, 2, 512]          65,536\n",
      "           Linear-48               [-1, 2, 512]          65,536\n",
      "           Linear-49               [-1, 2, 512]          65,536\n",
      "          Dropout-50               [-1, 2, 512]               0\n",
      "           Linear-51               [-1, 2, 128]          65,664\n",
      " HSISelfAttention-52               [-1, 2, 128]               0\n",
      "        LayerNorm-53               [-1, 2, 128]             256\n",
      "           Linear-54               [-1, 2, 256]          33,024\n",
      "             GELU-55               [-1, 2, 256]               0\n",
      "          Dropout-56               [-1, 2, 256]               0\n",
      "           Linear-57               [-1, 2, 128]          32,896\n",
      "          Dropout-58               [-1, 2, 128]               0\n",
      "              Mlp-59               [-1, 2, 128]               0\n",
      "            Block-60               [-1, 2, 128]               0\n",
      "        LayerNorm-61               [-1, 2, 128]             256\n",
      "        LayerNorm-62               [-1, 2, 128]             256\n",
      "           Linear-63               [-1, 2, 512]          65,536\n",
      "           Linear-64               [-1, 2, 512]          65,536\n",
      "           Linear-65               [-1, 2, 512]          65,536\n",
      "          Dropout-66               [-1, 2, 512]               0\n",
      "           Linear-67               [-1, 2, 128]          65,664\n",
      " HSISelfAttention-68               [-1, 2, 128]               0\n",
      "        LayerNorm-69               [-1, 2, 128]             256\n",
      "           Linear-70               [-1, 2, 256]          33,024\n",
      "             GELU-71               [-1, 2, 256]               0\n",
      "          Dropout-72               [-1, 2, 256]               0\n",
      "           Linear-73               [-1, 2, 128]          32,896\n",
      "          Dropout-74               [-1, 2, 128]               0\n",
      "              Mlp-75               [-1, 2, 128]               0\n",
      "            Block-76               [-1, 2, 128]               0\n",
      "        LayerNorm-77               [-1, 2, 128]             256\n",
      "TransformerEncoder-78               [-1, 2, 128]               0\n",
      "        LayerNorm-79             [-1, 145, 128]             256\n",
      "        LayerNorm-80             [-1, 145, 128]             256\n",
      "           Linear-81               [-1, 2, 512]          65,536\n",
      "           Linear-82             [-1, 145, 512]          65,536\n",
      "           Linear-83             [-1, 145, 512]          65,536\n",
      "          Dropout-84               [-1, 1, 512]               0\n",
      "           Linear-85               [-1, 1, 128]          65,664\n",
      "HSICrossAttention-86               [-1, 1, 128]               0\n",
      "        LayerNorm-87             [-1, 145, 128]             256\n",
      "           Linear-88             [-1, 145, 256]          33,024\n",
      "             GELU-89             [-1, 145, 256]               0\n",
      "          Dropout-90             [-1, 145, 256]               0\n",
      "           Linear-91             [-1, 145, 128]          32,896\n",
      "          Dropout-92             [-1, 145, 128]               0\n",
      "              Mlp-93             [-1, 145, 128]               0\n",
      "           BlockC-94             [-1, 145, 128]               0\n",
      "        LayerNorm-95             [-1, 145, 128]             256\n",
      "TransformerEncoderC-96             [-1, 145, 128]               0\n",
      "        LayerNorm-97               [-1, 2, 128]             256\n",
      "        LayerNorm-98               [-1, 2, 128]             256\n",
      "           Linear-99             [-1, 145, 512]          65,536\n",
      "          Linear-100               [-1, 2, 512]          65,536\n",
      "          Linear-101               [-1, 2, 512]          65,536\n",
      "         Dropout-102               [-1, 1, 512]               0\n",
      "          Linear-103               [-1, 1, 128]          65,664\n",
      "HSICrossAttention-104               [-1, 1, 128]               0\n",
      "       LayerNorm-105               [-1, 2, 128]             256\n",
      "          Linear-106               [-1, 2, 256]          33,024\n",
      "            GELU-107               [-1, 2, 256]               0\n",
      "         Dropout-108               [-1, 2, 256]               0\n",
      "          Linear-109               [-1, 2, 128]          32,896\n",
      "         Dropout-110               [-1, 2, 128]               0\n",
      "             Mlp-111               [-1, 2, 128]               0\n",
      "          BlockC-112               [-1, 2, 128]               0\n",
      "       LayerNorm-113               [-1, 2, 128]             256\n",
      "TransformerEncoderC-114               [-1, 2, 128]               0\n",
      "          Linear-115                   [-1, 15]           1,935\n",
      "          Linear-116                   [-1, 15]           1,935\n",
      "================================================================\n",
      "Total params: 2,013,442\n",
      "Trainable params: 2,013,442\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.60\n",
      "Forward/backward pass size (MB): 13.74\n",
      "Params size (MB): 7.68\n",
      "Estimated Total Size (MB): 25.02\n",
      "----------------------------------------------------------------\n",
      "Epoch [1/200], Training Loss: 2.7192, Training Accuracy: 18.86%, Test Loss: 2.6362, Test Accuracy: 13.27%\n",
      "\n",
      "\n",
      "Test accuracy increased (-inf --> 13.2738). Saving model ...\n",
      "Epoch [2/200], Training Loss: 2.3775, Training Accuracy: 23.20%, Test Loss: 2.4324, Test Accuracy: 15.52%\n",
      "\n",
      "\n",
      "Test accuracy increased (13.2738 --> 15.5202). Saving model ...\n",
      "Epoch [3/200], Training Loss: 2.0734, Training Accuracy: 39.27%, Test Loss: 1.9678, Test Accuracy: 32.66%\n",
      "\n",
      "\n",
      "Test accuracy increased (15.5202 --> 32.6638). Saving model ...\n",
      "Epoch [4/200], Training Loss: 1.7674, Training Accuracy: 46.75%, Test Loss: 1.8703, Test Accuracy: 36.57%\n",
      "\n",
      "\n",
      "Test accuracy increased (32.6638 --> 36.5664). Saving model ...\n",
      "Epoch [5/200], Training Loss: 1.5171, Training Accuracy: 52.08%, Test Loss: 1.7348, Test Accuracy: 42.16%\n",
      "\n",
      "\n",
      "Test accuracy increased (36.5664 --> 42.1579). Saving model ...\n",
      "Epoch [6/200], Training Loss: 1.3893, Training Accuracy: 54.17%, Test Loss: 1.5781, Test Accuracy: 44.68%\n",
      "\n",
      "\n",
      "Test accuracy increased (42.1579 --> 44.6831). Saving model ...\n",
      "Epoch [7/200], Training Loss: 1.2685, Training Accuracy: 63.31%, Test Loss: 1.4350, Test Accuracy: 51.02%\n",
      "\n",
      "\n",
      "Test accuracy increased (44.6831 --> 51.0207). Saving model ...\n",
      "Epoch [8/200], Training Loss: 1.0892, Training Accuracy: 66.42%, Test Loss: 1.4000, Test Accuracy: 52.35%\n",
      "\n",
      "\n",
      "Test accuracy increased (51.0207 --> 52.3489). Saving model ...\n",
      "Epoch [9/200], Training Loss: 0.9751, Training Accuracy: 70.87%, Test Loss: 1.2315, Test Accuracy: 59.51%\n",
      "\n",
      "\n",
      "Test accuracy increased (52.3489 --> 59.5064). Saving model ...\n",
      "Epoch [10/200], Training Loss: 0.8451, Training Accuracy: 76.80%, Test Loss: 1.1250, Test Accuracy: 60.84%\n",
      "\n",
      "\n",
      "Test accuracy increased (59.5064 --> 60.8428). Saving model ...\n",
      "Epoch [11/200], Training Loss: 0.7679, Training Accuracy: 78.14%, Test Loss: 1.0856, Test Accuracy: 63.57%\n",
      "\n",
      "\n",
      "Test accuracy increased (60.8428 --> 63.5730). Saving model ...\n",
      "Epoch [12/200], Training Loss: 0.6880, Training Accuracy: 78.00%, Test Loss: 1.0891, Test Accuracy: 64.04%\n",
      "\n",
      "\n",
      "Test accuracy increased (63.5730 --> 64.0403). Saving model ...\n",
      "Epoch [13/200], Training Loss: 0.6546, Training Accuracy: 84.25%, Test Loss: 0.9827, Test Accuracy: 66.70%\n",
      "\n",
      "\n",
      "Test accuracy increased (64.0403 --> 66.7049). Saving model ...\n",
      "Epoch [14/200], Training Loss: 0.5165, Training Accuracy: 77.90%, Test Loss: 1.1740, Test Accuracy: 62.29%\n",
      "Epoch [15/200], Training Loss: 0.5052, Training Accuracy: 85.28%, Test Loss: 0.9571, Test Accuracy: 68.60%\n",
      "\n",
      "\n",
      "Test accuracy increased (66.7049 --> 68.5988). Saving model ...\n",
      "Epoch [16/200], Training Loss: 0.3994, Training Accuracy: 91.42%, Test Loss: 0.8551, Test Accuracy: 72.79%\n",
      "\n",
      "\n",
      "Test accuracy increased (68.5988 --> 72.7884). Saving model ...\n",
      "Epoch [17/200], Training Loss: 0.3560, Training Accuracy: 85.66%, Test Loss: 0.9282, Test Accuracy: 69.36%\n",
      "Epoch [18/200], Training Loss: 0.3630, Training Accuracy: 88.95%, Test Loss: 0.9023, Test Accuracy: 70.10%\n",
      "Epoch [19/200], Training Loss: 0.3387, Training Accuracy: 93.29%, Test Loss: 0.7667, Test Accuracy: 75.52%\n",
      "\n",
      "\n",
      "Test accuracy increased (72.7884 --> 75.5186). Saving model ...\n",
      "Epoch [20/200], Training Loss: 0.3318, Training Accuracy: 85.88%, Test Loss: 0.9373, Test Accuracy: 70.59%\n",
      "Epoch [21/200], Training Loss: 0.3571, Training Accuracy: 87.11%, Test Loss: 0.9228, Test Accuracy: 70.19%\n",
      "Epoch [22/200], Training Loss: 0.2702, Training Accuracy: 94.63%, Test Loss: 0.7708, Test Accuracy: 74.94%\n",
      "Epoch [23/200], Training Loss: 0.2170, Training Accuracy: 94.95%, Test Loss: 0.7503, Test Accuracy: 76.14%\n",
      "\n",
      "\n",
      "Test accuracy increased (75.5186 --> 76.1417). Saving model ...\n",
      "Epoch [24/200], Training Loss: 0.2064, Training Accuracy: 96.57%, Test Loss: 0.6838, Test Accuracy: 78.37%\n",
      "\n",
      "\n",
      "Test accuracy increased (76.1417 --> 78.3717). Saving model ...\n",
      "Epoch [25/200], Training Loss: 0.1556, Training Accuracy: 96.75%, Test Loss: 0.7154, Test Accuracy: 78.77%\n",
      "\n",
      "\n",
      "Test accuracy increased (78.3717 --> 78.7735). Saving model ...\n",
      "Epoch [26/200], Training Loss: 0.1867, Training Accuracy: 94.70%, Test Loss: 0.7766, Test Accuracy: 76.35%\n",
      "Epoch [27/200], Training Loss: 0.1705, Training Accuracy: 94.21%, Test Loss: 0.7903, Test Accuracy: 76.25%\n",
      "Epoch [28/200], Training Loss: 0.1748, Training Accuracy: 96.47%, Test Loss: 0.7362, Test Accuracy: 78.43%\n",
      "Epoch [29/200], Training Loss: 0.1994, Training Accuracy: 97.95%, Test Loss: 0.6666, Test Accuracy: 79.59%\n",
      "\n",
      "\n",
      "Test accuracy increased (78.7735 --> 79.5933). Saving model ...\n",
      "Epoch [30/200], Training Loss: 0.1488, Training Accuracy: 96.26%, Test Loss: 0.8034, Test Accuracy: 76.85%\n",
      "Epoch [31/200], Training Loss: 0.1304, Training Accuracy: 97.78%, Test Loss: 0.7206, Test Accuracy: 79.11%\n",
      "Epoch [32/200], Training Loss: 0.1234, Training Accuracy: 97.53%, Test Loss: 0.7368, Test Accuracy: 78.98%\n",
      "Epoch [33/200], Training Loss: 0.0904, Training Accuracy: 97.70%, Test Loss: 0.6758, Test Accuracy: 80.13%\n",
      "\n",
      "\n",
      "Test accuracy increased (79.5933 --> 80.1263). Saving model ...\n",
      "Epoch [34/200], Training Loss: 0.1162, Training Accuracy: 98.09%, Test Loss: 0.6727, Test Accuracy: 80.13%\n",
      "\n",
      "\n",
      "Test accuracy increased (80.1263 --> 80.1345). Saving model ...\n",
      "Epoch [35/200], Training Loss: 0.0677, Training Accuracy: 99.51%, Test Loss: 0.6093, Test Accuracy: 83.06%\n",
      "\n",
      "\n",
      "Test accuracy increased (80.1345 --> 83.0614). Saving model ...\n",
      "Epoch [36/200], Training Loss: 0.0422, Training Accuracy: 98.83%, Test Loss: 0.7054, Test Accuracy: 80.67%\n",
      "Epoch [37/200], Training Loss: 0.0648, Training Accuracy: 97.00%, Test Loss: 0.8039, Test Accuracy: 78.02%\n",
      "Epoch [38/200], Training Loss: 0.1031, Training Accuracy: 96.82%, Test Loss: 0.7307, Test Accuracy: 78.96%\n",
      "Epoch [39/200], Training Loss: 0.1170, Training Accuracy: 98.31%, Test Loss: 0.7504, Test Accuracy: 79.26%\n",
      "Epoch [40/200], Training Loss: 0.0946, Training Accuracy: 97.53%, Test Loss: 0.8259, Test Accuracy: 77.92%\n",
      "Epoch [41/200], Training Loss: 0.0878, Training Accuracy: 98.87%, Test Loss: 0.6948, Test Accuracy: 80.92%\n",
      "Epoch [42/200], Training Loss: 0.1099, Training Accuracy: 98.09%, Test Loss: 0.7283, Test Accuracy: 80.36%\n",
      "Epoch [43/200], Training Loss: 0.0820, Training Accuracy: 97.85%, Test Loss: 0.7202, Test Accuracy: 80.49%\n",
      "Epoch [44/200], Training Loss: 0.0595, Training Accuracy: 99.36%, Test Loss: 0.6335, Test Accuracy: 82.26%\n",
      "Epoch [45/200], Training Loss: 0.0625, Training Accuracy: 98.16%, Test Loss: 0.7755, Test Accuracy: 78.98%\n",
      "Epoch [46/200], Training Loss: 0.0746, Training Accuracy: 99.05%, Test Loss: 0.6954, Test Accuracy: 81.14%\n",
      "Epoch [47/200], Training Loss: 0.1812, Training Accuracy: 93.22%, Test Loss: 0.8920, Test Accuracy: 76.46%\n",
      "Epoch [48/200], Training Loss: 0.2004, Training Accuracy: 98.59%, Test Loss: 0.7195, Test Accuracy: 79.72%\n",
      "Epoch [49/200], Training Loss: 0.1174, Training Accuracy: 96.65%, Test Loss: 0.9046, Test Accuracy: 76.08%\n",
      "Epoch [50/200], Training Loss: 0.1683, Training Accuracy: 92.23%, Test Loss: 1.0162, Test Accuracy: 73.81%\n",
      "Epoch [51/200], Training Loss: 0.0950, Training Accuracy: 98.98%, Test Loss: 0.7114, Test Accuracy: 81.01%\n",
      "Epoch [52/200], Training Loss: 0.0606, Training Accuracy: 99.40%, Test Loss: 0.6732, Test Accuracy: 82.13%\n",
      "Epoch [53/200], Training Loss: 0.0694, Training Accuracy: 99.40%, Test Loss: 0.6893, Test Accuracy: 81.63%\n",
      "Epoch [54/200], Training Loss: 0.0485, Training Accuracy: 99.40%, Test Loss: 0.6984, Test Accuracy: 81.50%\n",
      "Epoch [55/200], Training Loss: 0.0452, Training Accuracy: 97.35%, Test Loss: 0.7834, Test Accuracy: 80.23%\n",
      "Epoch [56/200], Training Loss: 0.0649, Training Accuracy: 98.62%, Test Loss: 0.6951, Test Accuracy: 81.48%\n",
      "Epoch [57/200], Training Loss: 0.0560, Training Accuracy: 97.32%, Test Loss: 0.7610, Test Accuracy: 79.27%\n",
      "Epoch [58/200], Training Loss: 0.0832, Training Accuracy: 98.66%, Test Loss: 0.7458, Test Accuracy: 80.90%\n",
      "Epoch [59/200], Training Loss: 0.0765, Training Accuracy: 98.62%, Test Loss: 0.7483, Test Accuracy: 81.02%\n",
      "Epoch [60/200], Training Loss: 0.0616, Training Accuracy: 98.45%, Test Loss: 0.7875, Test Accuracy: 80.54%\n",
      "Epoch [61/200], Training Loss: 0.0604, Training Accuracy: 99.36%, Test Loss: 0.8116, Test Accuracy: 80.77%\n",
      "Epoch [62/200], Training Loss: 0.0887, Training Accuracy: 95.90%, Test Loss: 0.9651, Test Accuracy: 76.23%\n",
      "Epoch [63/200], Training Loss: 0.0973, Training Accuracy: 97.60%, Test Loss: 0.8005, Test Accuracy: 80.09%\n",
      "Epoch [64/200], Training Loss: 0.1529, Training Accuracy: 98.34%, Test Loss: 0.8077, Test Accuracy: 79.73%\n",
      "Epoch [65/200], Training Loss: 0.0810, Training Accuracy: 97.46%, Test Loss: 0.7688, Test Accuracy: 80.20%\n",
      "Epoch [66/200], Training Loss: 0.1092, Training Accuracy: 97.25%, Test Loss: 0.8883, Test Accuracy: 78.18%\n",
      "Epoch [67/200], Training Loss: 0.0756, Training Accuracy: 99.08%, Test Loss: 0.7197, Test Accuracy: 82.24%\n",
      "Epoch [68/200], Training Loss: 0.0447, Training Accuracy: 98.94%, Test Loss: 0.7498, Test Accuracy: 81.17%\n",
      "Epoch [69/200], Training Loss: 0.0502, Training Accuracy: 99.33%, Test Loss: 0.7147, Test Accuracy: 82.32%\n",
      "Epoch [70/200], Training Loss: 0.0528, Training Accuracy: 98.80%, Test Loss: 0.7485, Test Accuracy: 81.73%\n",
      "Epoch [71/200], Training Loss: 0.0329, Training Accuracy: 99.19%, Test Loss: 0.7490, Test Accuracy: 81.78%\n",
      "Epoch [72/200], Training Loss: 0.0298, Training Accuracy: 99.36%, Test Loss: 0.7671, Test Accuracy: 81.76%\n",
      "Epoch [73/200], Training Loss: 0.0279, Training Accuracy: 99.22%, Test Loss: 0.7643, Test Accuracy: 81.78%\n",
      "Epoch [74/200], Training Loss: 0.0545, Training Accuracy: 99.54%, Test Loss: 0.7435, Test Accuracy: 82.09%\n",
      "Epoch [75/200], Training Loss: 0.0924, Training Accuracy: 98.27%, Test Loss: 0.8323, Test Accuracy: 79.36%\n",
      "Epoch [76/200], Training Loss: 0.0818, Training Accuracy: 98.83%, Test Loss: 0.7701, Test Accuracy: 80.81%\n",
      "Epoch [77/200], Training Loss: 0.1180, Training Accuracy: 97.92%, Test Loss: 0.8872, Test Accuracy: 78.44%\n",
      "Epoch [78/200], Training Loss: 0.0888, Training Accuracy: 98.52%, Test Loss: 0.7896, Test Accuracy: 80.61%\n",
      "Epoch [79/200], Training Loss: 0.0661, Training Accuracy: 99.08%, Test Loss: 0.7801, Test Accuracy: 80.93%\n",
      "Epoch [80/200], Training Loss: 0.0636, Training Accuracy: 97.99%, Test Loss: 0.8508, Test Accuracy: 79.47%\n",
      "Epoch [81/200], Training Loss: 0.0979, Training Accuracy: 99.61%, Test Loss: 0.7575, Test Accuracy: 81.88%\n",
      "Epoch [82/200], Training Loss: 0.0492, Training Accuracy: 99.22%, Test Loss: 0.7335, Test Accuracy: 81.41%\n",
      "Epoch [83/200], Training Loss: 0.0264, Training Accuracy: 99.72%, Test Loss: 0.7257, Test Accuracy: 82.00%\n",
      "Epoch [84/200], Training Loss: 0.0229, Training Accuracy: 99.82%, Test Loss: 0.6740, Test Accuracy: 84.08%\n",
      "\n",
      "\n",
      "Test accuracy increased (83.0614 --> 84.0781). Saving model ...\n",
      "Epoch [85/200], Training Loss: 0.0096, Training Accuracy: 99.68%, Test Loss: 0.7003, Test Accuracy: 83.41%\n",
      "Epoch [86/200], Training Loss: 0.0126, Training Accuracy: 99.79%, Test Loss: 0.7144, Test Accuracy: 83.50%\n",
      "Epoch [87/200], Training Loss: 0.0398, Training Accuracy: 98.38%, Test Loss: 0.8103, Test Accuracy: 80.29%\n",
      "Epoch [88/200], Training Loss: 0.0315, Training Accuracy: 99.82%, Test Loss: 0.6524, Test Accuracy: 84.76%\n",
      "\n",
      "\n",
      "Test accuracy increased (84.0781 --> 84.7585). Saving model ...\n",
      "Epoch [89/200], Training Loss: 0.0107, Training Accuracy: 99.75%, Test Loss: 0.7251, Test Accuracy: 82.98%\n",
      "Epoch [90/200], Training Loss: 0.0136, Training Accuracy: 99.72%, Test Loss: 0.7068, Test Accuracy: 83.20%\n",
      "Epoch [91/200], Training Loss: 0.0239, Training Accuracy: 99.26%, Test Loss: 0.7875, Test Accuracy: 81.63%\n",
      "Epoch [92/200], Training Loss: 0.0660, Training Accuracy: 97.00%, Test Loss: 0.9188, Test Accuracy: 78.25%\n",
      "Epoch [93/200], Training Loss: 0.1255, Training Accuracy: 96.36%, Test Loss: 0.9062, Test Accuracy: 78.90%\n",
      "Epoch [94/200], Training Loss: 0.2233, Training Accuracy: 96.75%, Test Loss: 0.9317, Test Accuracy: 77.13%\n",
      "Epoch [95/200], Training Loss: 0.1608, Training Accuracy: 98.41%, Test Loss: 0.6958, Test Accuracy: 81.74%\n",
      "Epoch [96/200], Training Loss: 0.0937, Training Accuracy: 95.30%, Test Loss: 0.9911, Test Accuracy: 75.44%\n",
      "Epoch [97/200], Training Loss: 0.0783, Training Accuracy: 98.41%, Test Loss: 0.7312, Test Accuracy: 82.00%\n",
      "Epoch [98/200], Training Loss: 0.0291, Training Accuracy: 99.72%, Test Loss: 0.6888, Test Accuracy: 83.21%\n",
      "Epoch [99/200], Training Loss: 0.0161, Training Accuracy: 99.79%, Test Loss: 0.6934, Test Accuracy: 83.76%\n",
      "Epoch [100/200], Training Loss: 0.0137, Training Accuracy: 99.89%, Test Loss: 0.6510, Test Accuracy: 84.43%\n",
      "Epoch [101/200], Training Loss: 0.0104, Training Accuracy: 99.93%, Test Loss: 0.6103, Test Accuracy: 85.73%\n",
      "\n",
      "\n",
      "Test accuracy increased (84.7585 --> 85.7260). Saving model ...\n",
      "Epoch [102/200], Training Loss: 0.0103, Training Accuracy: 99.79%, Test Loss: 0.6818, Test Accuracy: 84.21%\n",
      "Epoch [103/200], Training Loss: 0.0103, Training Accuracy: 99.93%, Test Loss: 0.6971, Test Accuracy: 83.90%\n",
      "Epoch [104/200], Training Loss: 0.0094, Training Accuracy: 99.72%, Test Loss: 0.7168, Test Accuracy: 83.56%\n",
      "Epoch [105/200], Training Loss: 0.0200, Training Accuracy: 99.68%, Test Loss: 0.6983, Test Accuracy: 84.46%\n",
      "Epoch [106/200], Training Loss: 0.0319, Training Accuracy: 98.55%, Test Loss: 0.8760, Test Accuracy: 80.18%\n",
      "Epoch [107/200], Training Loss: 0.0448, Training Accuracy: 99.61%, Test Loss: 0.8389, Test Accuracy: 81.55%\n",
      "Epoch [108/200], Training Loss: 0.1219, Training Accuracy: 95.55%, Test Loss: 1.0238, Test Accuracy: 75.31%\n",
      "Epoch [109/200], Training Loss: 0.1300, Training Accuracy: 97.67%, Test Loss: 0.9359, Test Accuracy: 78.21%\n",
      "Epoch [110/200], Training Loss: 0.0887, Training Accuracy: 98.31%, Test Loss: 0.8092, Test Accuracy: 80.66%\n",
      "Epoch [111/200], Training Loss: 0.0789, Training Accuracy: 98.06%, Test Loss: 0.8558, Test Accuracy: 79.88%\n",
      "Epoch [112/200], Training Loss: 0.0521, Training Accuracy: 99.40%, Test Loss: 0.7674, Test Accuracy: 82.09%\n",
      "Epoch [113/200], Training Loss: 0.0430, Training Accuracy: 99.15%, Test Loss: 0.7490, Test Accuracy: 82.32%\n",
      "Epoch [114/200], Training Loss: 0.0378, Training Accuracy: 99.72%, Test Loss: 0.6623, Test Accuracy: 84.36%\n",
      "Epoch [115/200], Training Loss: 0.0214, Training Accuracy: 98.48%, Test Loss: 0.8329, Test Accuracy: 80.96%\n",
      "Epoch [116/200], Training Loss: 0.0344, Training Accuracy: 99.75%, Test Loss: 0.7054, Test Accuracy: 83.99%\n",
      "Epoch [117/200], Training Loss: 0.0337, Training Accuracy: 99.82%, Test Loss: 0.7008, Test Accuracy: 83.86%\n",
      "Epoch [118/200], Training Loss: 0.0492, Training Accuracy: 99.68%, Test Loss: 0.6715, Test Accuracy: 84.24%\n",
      "Epoch [119/200], Training Loss: 0.0624, Training Accuracy: 98.23%, Test Loss: 0.8871, Test Accuracy: 79.60%\n",
      "Epoch [120/200], Training Loss: 0.0650, Training Accuracy: 98.83%, Test Loss: 0.7454, Test Accuracy: 82.72%\n",
      "Epoch [121/200], Training Loss: 0.0702, Training Accuracy: 97.88%, Test Loss: 0.8899, Test Accuracy: 79.83%\n",
      "Epoch [122/200], Training Loss: 0.0687, Training Accuracy: 98.87%, Test Loss: 0.8634, Test Accuracy: 80.50%\n",
      "Epoch [123/200], Training Loss: 0.1860, Training Accuracy: 96.79%, Test Loss: 0.8554, Test Accuracy: 78.42%\n",
      "Epoch [124/200], Training Loss: 0.0893, Training Accuracy: 98.87%, Test Loss: 0.7752, Test Accuracy: 82.36%\n",
      "Epoch [125/200], Training Loss: 0.0584, Training Accuracy: 97.56%, Test Loss: 0.8212, Test Accuracy: 80.89%\n",
      "Epoch [126/200], Training Loss: 0.0475, Training Accuracy: 99.29%, Test Loss: 0.8044, Test Accuracy: 82.28%\n",
      "Epoch [127/200], Training Loss: 0.1034, Training Accuracy: 98.76%, Test Loss: 0.7562, Test Accuracy: 82.27%\n",
      "Epoch [128/200], Training Loss: 0.1112, Training Accuracy: 98.69%, Test Loss: 0.7633, Test Accuracy: 81.21%\n",
      "Epoch [129/200], Training Loss: 0.0447, Training Accuracy: 99.61%, Test Loss: 0.6702, Test Accuracy: 83.52%\n",
      "Epoch [130/200], Training Loss: 0.0182, Training Accuracy: 99.75%, Test Loss: 0.6468, Test Accuracy: 84.46%\n",
      "Epoch [131/200], Training Loss: 0.0063, Training Accuracy: 100.00%, Test Loss: 0.6032, Test Accuracy: 85.83%\n",
      "\n",
      "\n",
      "Test accuracy increased (85.7260 --> 85.8326). Saving model ...\n",
      "Epoch [132/200], Training Loss: 0.0033, Training Accuracy: 99.93%, Test Loss: 0.6035, Test Accuracy: 85.82%\n",
      "Epoch [133/200], Training Loss: 0.0051, Training Accuracy: 100.00%, Test Loss: 0.6259, Test Accuracy: 86.19%\n",
      "\n",
      "\n",
      "Test accuracy increased (85.8326 --> 86.1851). Saving model ...\n",
      "Epoch [134/200], Training Loss: 0.0023, Training Accuracy: 100.00%, Test Loss: 0.5936, Test Accuracy: 86.38%\n",
      "\n",
      "\n",
      "Test accuracy increased (86.1851 --> 86.3819). Saving model ...\n",
      "Epoch [135/200], Training Loss: 0.0036, Training Accuracy: 99.93%, Test Loss: 0.6322, Test Accuracy: 85.87%\n",
      "Epoch [136/200], Training Loss: 0.0042, Training Accuracy: 100.00%, Test Loss: 0.5956, Test Accuracy: 86.32%\n",
      "Epoch [137/200], Training Loss: 0.0019, Training Accuracy: 100.00%, Test Loss: 0.5937, Test Accuracy: 86.25%\n",
      "Epoch [138/200], Training Loss: 0.0010, Training Accuracy: 100.00%, Test Loss: 0.5737, Test Accuracy: 86.86%\n",
      "\n",
      "\n",
      "Test accuracy increased (86.3819 --> 86.8574). Saving model ...\n",
      "Epoch [139/200], Training Loss: 0.0007, Training Accuracy: 100.00%, Test Loss: 0.5747, Test Accuracy: 86.97%\n",
      "\n",
      "\n",
      "Test accuracy increased (86.8574 --> 86.9722). Saving model ...\n",
      "Epoch [140/200], Training Loss: 0.0007, Training Accuracy: 100.00%, Test Loss: 0.5753, Test Accuracy: 87.05%\n",
      "\n",
      "\n",
      "Test accuracy increased (86.9722 --> 87.0460). Saving model ...\n",
      "Epoch [141/200], Training Loss: 0.0007, Training Accuracy: 100.00%, Test Loss: 0.5740, Test Accuracy: 87.19%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.0460 --> 87.1936). Saving model ...\n",
      "Epoch [142/200], Training Loss: 0.0006, Training Accuracy: 100.00%, Test Loss: 0.5795, Test Accuracy: 87.14%\n",
      "Epoch [143/200], Training Loss: 0.0005, Training Accuracy: 100.00%, Test Loss: 0.5793, Test Accuracy: 87.17%\n",
      "Epoch [144/200], Training Loss: 0.0005, Training Accuracy: 100.00%, Test Loss: 0.5903, Test Accuracy: 87.13%\n",
      "Epoch [145/200], Training Loss: 0.0014, Training Accuracy: 99.86%, Test Loss: 0.6470, Test Accuracy: 85.67%\n",
      "Epoch [146/200], Training Loss: 0.0058, Training Accuracy: 99.93%, Test Loss: 0.6094, Test Accuracy: 86.44%\n",
      "Epoch [147/200], Training Loss: 0.0076, Training Accuracy: 99.58%, Test Loss: 0.7327, Test Accuracy: 84.32%\n",
      "Epoch [148/200], Training Loss: 0.0120, Training Accuracy: 99.82%, Test Loss: 0.7287, Test Accuracy: 84.66%\n",
      "Epoch [149/200], Training Loss: 0.0161, Training Accuracy: 99.96%, Test Loss: 0.7159, Test Accuracy: 84.41%\n",
      "Epoch [150/200], Training Loss: 0.0327, Training Accuracy: 98.23%, Test Loss: 0.8377, Test Accuracy: 81.20%\n",
      "Epoch [151/200], Training Loss: 0.2581, Training Accuracy: 89.90%, Test Loss: 1.1240, Test Accuracy: 72.15%\n",
      "Epoch [152/200], Training Loss: 0.3520, Training Accuracy: 90.82%, Test Loss: 1.0369, Test Accuracy: 73.19%\n",
      "Epoch [153/200], Training Loss: 0.1514, Training Accuracy: 98.16%, Test Loss: 0.7723, Test Accuracy: 80.85%\n",
      "Epoch [154/200], Training Loss: 0.0626, Training Accuracy: 99.40%, Test Loss: 0.7121, Test Accuracy: 82.52%\n",
      "Epoch [155/200], Training Loss: 0.0386, Training Accuracy: 99.58%, Test Loss: 0.6501, Test Accuracy: 84.55%\n",
      "Epoch [156/200], Training Loss: 0.0328, Training Accuracy: 99.15%, Test Loss: 0.7509, Test Accuracy: 82.68%\n",
      "Epoch [157/200], Training Loss: 0.0321, Training Accuracy: 99.40%, Test Loss: 0.6887, Test Accuracy: 83.55%\n",
      "Epoch [158/200], Training Loss: 0.1159, Training Accuracy: 98.48%, Test Loss: 0.7683, Test Accuracy: 81.46%\n",
      "Epoch [159/200], Training Loss: 0.0720, Training Accuracy: 98.66%, Test Loss: 0.7808, Test Accuracy: 81.54%\n",
      "Epoch [160/200], Training Loss: 0.0490, Training Accuracy: 98.55%, Test Loss: 0.7963, Test Accuracy: 81.76%\n",
      "Epoch [161/200], Training Loss: 0.0443, Training Accuracy: 98.83%, Test Loss: 0.8243, Test Accuracy: 81.19%\n",
      "Epoch [162/200], Training Loss: 0.0432, Training Accuracy: 98.48%, Test Loss: 0.9481, Test Accuracy: 80.09%\n",
      "Epoch [163/200], Training Loss: 0.0534, Training Accuracy: 98.34%, Test Loss: 0.8133, Test Accuracy: 80.82%\n",
      "Epoch [164/200], Training Loss: 0.0463, Training Accuracy: 99.61%, Test Loss: 0.7130, Test Accuracy: 83.39%\n",
      "Epoch [165/200], Training Loss: 0.0245, Training Accuracy: 99.86%, Test Loss: 0.6528, Test Accuracy: 85.11%\n",
      "Epoch [166/200], Training Loss: 0.0108, Training Accuracy: 99.79%, Test Loss: 0.7510, Test Accuracy: 83.87%\n",
      "Epoch [167/200], Training Loss: 0.0120, Training Accuracy: 99.79%, Test Loss: 0.6833, Test Accuracy: 84.45%\n",
      "Epoch [168/200], Training Loss: 0.0140, Training Accuracy: 99.89%, Test Loss: 0.6890, Test Accuracy: 84.76%\n",
      "Epoch [169/200], Training Loss: 0.0221, Training Accuracy: 99.89%, Test Loss: 0.6918, Test Accuracy: 85.22%\n",
      "Epoch [170/200], Training Loss: 0.0052, Training Accuracy: 99.89%, Test Loss: 0.6241, Test Accuracy: 86.28%\n",
      "Epoch [171/200], Training Loss: 0.0055, Training Accuracy: 100.00%, Test Loss: 0.6178, Test Accuracy: 86.59%\n",
      "Epoch [172/200], Training Loss: 0.0017, Training Accuracy: 100.00%, Test Loss: 0.5947, Test Accuracy: 87.06%\n",
      "Epoch [173/200], Training Loss: 0.0014, Training Accuracy: 100.00%, Test Loss: 0.6062, Test Accuracy: 87.04%\n",
      "Epoch [174/200], Training Loss: 0.0007, Training Accuracy: 100.00%, Test Loss: 0.5925, Test Accuracy: 87.32%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.1936 --> 87.3248). Saving model ...\n",
      "Epoch [175/200], Training Loss: 0.0005, Training Accuracy: 100.00%, Test Loss: 0.5879, Test Accuracy: 87.47%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.3248 --> 87.4723). Saving model ...\n",
      "Epoch [176/200], Training Loss: 0.0005, Training Accuracy: 100.00%, Test Loss: 0.5838, Test Accuracy: 87.60%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.4723 --> 87.5953). Saving model ...\n",
      "Epoch [177/200], Training Loss: 0.0005, Training Accuracy: 100.00%, Test Loss: 0.5923, Test Accuracy: 87.49%\n",
      "Epoch [178/200], Training Loss: 0.0005, Training Accuracy: 100.00%, Test Loss: 0.5909, Test Accuracy: 87.57%\n",
      "Epoch [179/200], Training Loss: 0.0004, Training Accuracy: 100.00%, Test Loss: 0.5975, Test Accuracy: 87.46%\n",
      "Epoch [180/200], Training Loss: 0.0005, Training Accuracy: 100.00%, Test Loss: 0.5975, Test Accuracy: 87.35%\n",
      "Epoch [181/200], Training Loss: 0.0004, Training Accuracy: 100.00%, Test Loss: 0.5975, Test Accuracy: 87.53%\n",
      "Epoch [182/200], Training Loss: 0.0004, Training Accuracy: 100.00%, Test Loss: 0.5965, Test Accuracy: 87.63%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.5953 --> 87.6281). Saving model ...\n",
      "Epoch [183/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.5977, Test Accuracy: 87.69%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.6281 --> 87.6937). Saving model ...\n",
      "Epoch [184/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.5984, Test Accuracy: 87.67%\n",
      "Epoch [185/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.5967, Test Accuracy: 87.67%\n",
      "Epoch [186/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.5963, Test Accuracy: 87.80%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.6937 --> 87.8003). Saving model ...\n",
      "Epoch [187/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.5972, Test Accuracy: 87.82%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.8003 --> 87.8167). Saving model ...\n",
      "Epoch [188/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.5976, Test Accuracy: 87.84%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.8167 --> 87.8413). Saving model ...\n",
      "Epoch [189/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.5992, Test Accuracy: 87.91%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.8413 --> 87.9069). Saving model ...\n",
      "Epoch [190/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.5998, Test Accuracy: 87.87%\n",
      "Epoch [191/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.5964, Test Accuracy: 87.88%\n",
      "Epoch [192/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.5972, Test Accuracy: 87.96%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.9069 --> 87.9561). Saving model ...\n",
      "Epoch [193/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.5974, Test Accuracy: 87.89%\n",
      "Epoch [194/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.6010, Test Accuracy: 87.92%\n",
      "Epoch [195/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6012, Test Accuracy: 87.91%\n",
      "Epoch [196/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6012, Test Accuracy: 87.91%\n",
      "Epoch [197/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.6006, Test Accuracy: 87.91%\n",
      "Epoch [198/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.6057, Test Accuracy: 87.91%\n",
      "Epoch [199/200], Training Loss: 0.0005, Training Accuracy: 99.93%, Test Loss: 0.6379, Test Accuracy: 87.14%\n",
      "Epoch [200/200], Training Loss: 0.0070, Training Accuracy: 99.89%, Test Loss: 0.6166, Test Accuracy: 87.06%\n",
      "\n",
      "\n",
      "\n",
      "The train time (in seconds) is: 458.57934045791626\n",
      "Test Loss: 0.5972\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 81, 144, 1]           6,642\n",
      "            Linear-2             [-1, 144, 128]          10,496\n",
      "         LayerNorm-3             [-1, 144, 128]             256\n",
      "           Dropout-4             [-1, 145, 128]               0\n",
      "    PatchEmbedding-5             [-1, 145, 128]               0\n",
      "            Conv2d-6             [-1, 81, 1, 1]           6,642\n",
      "            Linear-7               [-1, 1, 128]          10,496\n",
      "         LayerNorm-8               [-1, 1, 128]             256\n",
      "           Dropout-9               [-1, 2, 128]               0\n",
      "   PatchEmbedding-10               [-1, 2, 128]               0\n",
      "        LayerNorm-11             [-1, 145, 128]             256\n",
      "        LayerNorm-12             [-1, 145, 128]             256\n",
      "           Linear-13             [-1, 145, 512]          65,536\n",
      "           Linear-14             [-1, 145, 512]          65,536\n",
      "           Linear-15             [-1, 145, 512]          65,536\n",
      "          Dropout-16             [-1, 145, 512]               0\n",
      "           Linear-17             [-1, 145, 128]          65,664\n",
      " HSISelfAttention-18             [-1, 145, 128]               0\n",
      "        LayerNorm-19             [-1, 145, 128]             256\n",
      "           Linear-20             [-1, 145, 256]          33,024\n",
      "             GELU-21             [-1, 145, 256]               0\n",
      "          Dropout-22             [-1, 145, 256]               0\n",
      "           Linear-23             [-1, 145, 128]          32,896\n",
      "          Dropout-24             [-1, 145, 128]               0\n",
      "              Mlp-25             [-1, 145, 128]               0\n",
      "            Block-26             [-1, 145, 128]               0\n",
      "        LayerNorm-27             [-1, 145, 128]             256\n",
      "        LayerNorm-28             [-1, 145, 128]             256\n",
      "           Linear-29             [-1, 145, 512]          65,536\n",
      "           Linear-30             [-1, 145, 512]          65,536\n",
      "           Linear-31             [-1, 145, 512]          65,536\n",
      "          Dropout-32             [-1, 145, 512]               0\n",
      "           Linear-33             [-1, 145, 128]          65,664\n",
      " HSISelfAttention-34             [-1, 145, 128]               0\n",
      "        LayerNorm-35             [-1, 145, 128]             256\n",
      "           Linear-36             [-1, 145, 256]          33,024\n",
      "             GELU-37             [-1, 145, 256]               0\n",
      "          Dropout-38             [-1, 145, 256]               0\n",
      "           Linear-39             [-1, 145, 128]          32,896\n",
      "          Dropout-40             [-1, 145, 128]               0\n",
      "              Mlp-41             [-1, 145, 128]               0\n",
      "            Block-42             [-1, 145, 128]               0\n",
      "        LayerNorm-43             [-1, 145, 128]             256\n",
      "TransformerEncoder-44             [-1, 145, 128]               0\n",
      "        LayerNorm-45               [-1, 2, 128]             256\n",
      "        LayerNorm-46               [-1, 2, 128]             256\n",
      "           Linear-47               [-1, 2, 512]          65,536\n",
      "           Linear-48               [-1, 2, 512]          65,536\n",
      "           Linear-49               [-1, 2, 512]          65,536\n",
      "          Dropout-50               [-1, 2, 512]               0\n",
      "           Linear-51               [-1, 2, 128]          65,664\n",
      " HSISelfAttention-52               [-1, 2, 128]               0\n",
      "        LayerNorm-53               [-1, 2, 128]             256\n",
      "           Linear-54               [-1, 2, 256]          33,024\n",
      "             GELU-55               [-1, 2, 256]               0\n",
      "          Dropout-56               [-1, 2, 256]               0\n",
      "           Linear-57               [-1, 2, 128]          32,896\n",
      "          Dropout-58               [-1, 2, 128]               0\n",
      "              Mlp-59               [-1, 2, 128]               0\n",
      "            Block-60               [-1, 2, 128]               0\n",
      "        LayerNorm-61               [-1, 2, 128]             256\n",
      "        LayerNorm-62               [-1, 2, 128]             256\n",
      "           Linear-63               [-1, 2, 512]          65,536\n",
      "           Linear-64               [-1, 2, 512]          65,536\n",
      "           Linear-65               [-1, 2, 512]          65,536\n",
      "          Dropout-66               [-1, 2, 512]               0\n",
      "           Linear-67               [-1, 2, 128]          65,664\n",
      " HSISelfAttention-68               [-1, 2, 128]               0\n",
      "        LayerNorm-69               [-1, 2, 128]             256\n",
      "           Linear-70               [-1, 2, 256]          33,024\n",
      "             GELU-71               [-1, 2, 256]               0\n",
      "          Dropout-72               [-1, 2, 256]               0\n",
      "           Linear-73               [-1, 2, 128]          32,896\n",
      "          Dropout-74               [-1, 2, 128]               0\n",
      "              Mlp-75               [-1, 2, 128]               0\n",
      "            Block-76               [-1, 2, 128]               0\n",
      "        LayerNorm-77               [-1, 2, 128]             256\n",
      "TransformerEncoder-78               [-1, 2, 128]               0\n",
      "        LayerNorm-79             [-1, 145, 128]             256\n",
      "        LayerNorm-80             [-1, 145, 128]             256\n",
      "           Linear-81               [-1, 2, 512]          65,536\n",
      "           Linear-82             [-1, 145, 512]          65,536\n",
      "           Linear-83             [-1, 145, 512]          65,536\n",
      "          Dropout-84               [-1, 1, 512]               0\n",
      "           Linear-85               [-1, 1, 128]          65,664\n",
      "HSICrossAttention-86               [-1, 1, 128]               0\n",
      "        LayerNorm-87             [-1, 145, 128]             256\n",
      "           Linear-88             [-1, 145, 256]          33,024\n",
      "             GELU-89             [-1, 145, 256]               0\n",
      "          Dropout-90             [-1, 145, 256]               0\n",
      "           Linear-91             [-1, 145, 128]          32,896\n",
      "          Dropout-92             [-1, 145, 128]               0\n",
      "              Mlp-93             [-1, 145, 128]               0\n",
      "           BlockC-94             [-1, 145, 128]               0\n",
      "        LayerNorm-95             [-1, 145, 128]             256\n",
      "TransformerEncoderC-96             [-1, 145, 128]               0\n",
      "        LayerNorm-97               [-1, 2, 128]             256\n",
      "        LayerNorm-98               [-1, 2, 128]             256\n",
      "           Linear-99             [-1, 145, 512]          65,536\n",
      "          Linear-100               [-1, 2, 512]          65,536\n",
      "          Linear-101               [-1, 2, 512]          65,536\n",
      "         Dropout-102               [-1, 1, 512]               0\n",
      "          Linear-103               [-1, 1, 128]          65,664\n",
      "HSICrossAttention-104               [-1, 1, 128]               0\n",
      "       LayerNorm-105               [-1, 2, 128]             256\n",
      "          Linear-106               [-1, 2, 256]          33,024\n",
      "            GELU-107               [-1, 2, 256]               0\n",
      "         Dropout-108               [-1, 2, 256]               0\n",
      "          Linear-109               [-1, 2, 128]          32,896\n",
      "         Dropout-110               [-1, 2, 128]               0\n",
      "             Mlp-111               [-1, 2, 128]               0\n",
      "          BlockC-112               [-1, 2, 128]               0\n",
      "       LayerNorm-113               [-1, 2, 128]             256\n",
      "TransformerEncoderC-114               [-1, 2, 128]               0\n",
      "          Linear-115                   [-1, 15]           1,935\n",
      "          Linear-116                   [-1, 15]           1,935\n",
      "================================================================\n",
      "Total params: 2,013,442\n",
      "Trainable params: 2,013,442\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.60\n",
      "Forward/backward pass size (MB): 13.74\n",
      "Params size (MB): 7.68\n",
      "Estimated Total Size (MB): 25.02\n",
      "----------------------------------------------------------------\n",
      "Epoch [1/200], Training Loss: 2.7453, Training Accuracy: 7.34%, Test Loss: 2.7102, Test Accuracy: 2.80%\n",
      "\n",
      "\n",
      "Test accuracy increased (-inf --> 2.8040). Saving model ...\n",
      "Epoch [2/200], Training Loss: 2.6076, Training Accuracy: 17.27%, Test Loss: 2.4692, Test Accuracy: 14.53%\n",
      "\n",
      "\n",
      "Test accuracy increased (2.8040 --> 14.5282). Saving model ...\n",
      "Epoch [3/200], Training Loss: 2.3337, Training Accuracy: 27.65%, Test Loss: 2.2617, Test Accuracy: 23.28%\n",
      "\n",
      "\n",
      "Test accuracy increased (14.5282 --> 23.2844). Saving model ...\n",
      "Epoch [4/200], Training Loss: 2.1050, Training Accuracy: 37.50%, Test Loss: 2.1714, Test Accuracy: 27.66%\n",
      "\n",
      "\n",
      "Test accuracy increased (23.2844 --> 27.6625). Saving model ...\n",
      "Epoch [5/200], Training Loss: 1.8777, Training Accuracy: 28.85%, Test Loss: 2.1145, Test Accuracy: 21.10%\n",
      "Epoch [6/200], Training Loss: 1.6733, Training Accuracy: 48.69%, Test Loss: 1.7501, Test Accuracy: 36.70%\n",
      "\n",
      "\n",
      "Test accuracy increased (27.6625 --> 36.6975). Saving model ...\n",
      "Epoch [7/200], Training Loss: 1.4248, Training Accuracy: 55.40%, Test Loss: 1.6528, Test Accuracy: 43.86%\n",
      "\n",
      "\n",
      "Test accuracy increased (36.6975 --> 43.8632). Saving model ...\n",
      "Epoch [8/200], Training Loss: 1.3025, Training Accuracy: 61.05%, Test Loss: 1.4914, Test Accuracy: 48.62%\n",
      "\n",
      "\n",
      "Test accuracy increased (43.8632 --> 48.6185). Saving model ...\n",
      "Epoch [9/200], Training Loss: 1.1239, Training Accuracy: 63.77%, Test Loss: 1.3534, Test Accuracy: 55.02%\n",
      "\n",
      "\n",
      "Test accuracy increased (48.6185 --> 55.0217). Saving model ...\n",
      "Epoch [10/200], Training Loss: 0.9844, Training Accuracy: 72.10%, Test Loss: 1.1563, Test Accuracy: 60.17%\n",
      "\n",
      "\n",
      "Test accuracy increased (55.0217 --> 60.1705). Saving model ...\n",
      "Epoch [11/200], Training Loss: 0.9163, Training Accuracy: 73.06%, Test Loss: 1.2228, Test Accuracy: 59.01%\n",
      "Epoch [12/200], Training Loss: 0.8173, Training Accuracy: 76.48%, Test Loss: 1.0570, Test Accuracy: 65.13%\n",
      "\n",
      "\n",
      "Test accuracy increased (60.1705 --> 65.1308). Saving model ...\n",
      "Epoch [13/200], Training Loss: 0.7095, Training Accuracy: 80.19%, Test Loss: 1.0026, Test Accuracy: 66.82%\n",
      "\n",
      "\n",
      "Test accuracy increased (65.1308 --> 66.8197). Saving model ...\n",
      "Epoch [14/200], Training Loss: 0.6263, Training Accuracy: 81.85%, Test Loss: 0.9822, Test Accuracy: 66.39%\n",
      "Epoch [15/200], Training Loss: 0.5965, Training Accuracy: 81.32%, Test Loss: 0.9880, Test Accuracy: 66.79%\n",
      "Epoch [16/200], Training Loss: 0.5441, Training Accuracy: 83.97%, Test Loss: 0.9087, Test Accuracy: 69.95%\n",
      "\n",
      "\n",
      "Test accuracy increased (66.8197 --> 69.9516). Saving model ...\n",
      "Epoch [17/200], Training Loss: 0.4956, Training Accuracy: 81.71%, Test Loss: 0.9591, Test Accuracy: 68.97%\n",
      "Epoch [18/200], Training Loss: 0.5253, Training Accuracy: 87.25%, Test Loss: 0.8852, Test Accuracy: 70.74%\n",
      "\n",
      "\n",
      "Test accuracy increased (69.9516 --> 70.7387). Saving model ...\n",
      "Epoch [19/200], Training Loss: 0.4063, Training Accuracy: 92.34%, Test Loss: 0.7408, Test Accuracy: 75.66%\n",
      "\n",
      "\n",
      "Test accuracy increased (70.7387 --> 75.6579). Saving model ...\n",
      "Epoch [20/200], Training Loss: 0.3795, Training Accuracy: 90.96%, Test Loss: 0.7707, Test Accuracy: 75.08%\n",
      "Epoch [21/200], Training Loss: 0.3119, Training Accuracy: 91.38%, Test Loss: 0.7057, Test Accuracy: 76.60%\n",
      "\n",
      "\n",
      "Test accuracy increased (75.6579 --> 76.6008). Saving model ...\n",
      "Epoch [22/200], Training Loss: 0.3176, Training Accuracy: 92.48%, Test Loss: 0.7095, Test Accuracy: 76.58%\n",
      "Epoch [23/200], Training Loss: 0.2764, Training Accuracy: 91.49%, Test Loss: 0.7865, Test Accuracy: 74.85%\n",
      "Epoch [24/200], Training Loss: 0.3023, Training Accuracy: 90.08%, Test Loss: 0.8936, Test Accuracy: 72.25%\n",
      "Epoch [25/200], Training Loss: 0.3097, Training Accuracy: 90.68%, Test Loss: 0.7802, Test Accuracy: 74.64%\n",
      "Epoch [26/200], Training Loss: 0.2679, Training Accuracy: 94.21%, Test Loss: 0.7159, Test Accuracy: 77.47%\n",
      "\n",
      "\n",
      "Test accuracy increased (76.6008 --> 77.4699). Saving model ...\n",
      "Epoch [27/200], Training Loss: 0.3113, Training Accuracy: 91.35%, Test Loss: 0.8161, Test Accuracy: 74.29%\n",
      "Epoch [28/200], Training Loss: 0.2404, Training Accuracy: 93.71%, Test Loss: 0.7380, Test Accuracy: 77.00%\n",
      "Epoch [29/200], Training Loss: 0.1785, Training Accuracy: 91.03%, Test Loss: 0.8229, Test Accuracy: 75.03%\n",
      "Epoch [30/200], Training Loss: 0.1987, Training Accuracy: 96.43%, Test Loss: 0.6212, Test Accuracy: 80.68%\n",
      "\n",
      "\n",
      "Test accuracy increased (77.4699 --> 80.6838). Saving model ...\n",
      "Epoch [31/200], Training Loss: 0.2547, Training Accuracy: 96.01%, Test Loss: 0.6375, Test Accuracy: 79.99%\n",
      "Epoch [32/200], Training Loss: 0.1195, Training Accuracy: 93.50%, Test Loss: 0.7417, Test Accuracy: 77.81%\n",
      "Epoch [33/200], Training Loss: 0.1581, Training Accuracy: 96.05%, Test Loss: 0.6754, Test Accuracy: 79.52%\n",
      "Epoch [34/200], Training Loss: 0.1115, Training Accuracy: 97.60%, Test Loss: 0.5862, Test Accuracy: 82.87%\n",
      "\n",
      "\n",
      "Test accuracy increased (80.6838 --> 82.8728). Saving model ...\n",
      "Epoch [35/200], Training Loss: 0.1222, Training Accuracy: 97.14%, Test Loss: 0.6477, Test Accuracy: 80.82%\n",
      "Epoch [36/200], Training Loss: 0.0904, Training Accuracy: 98.87%, Test Loss: 0.5887, Test Accuracy: 82.55%\n",
      "Epoch [37/200], Training Loss: 0.0986, Training Accuracy: 91.21%, Test Loss: 0.8604, Test Accuracy: 75.44%\n",
      "Epoch [38/200], Training Loss: 0.1931, Training Accuracy: 94.95%, Test Loss: 0.8495, Test Accuracy: 76.92%\n",
      "Epoch [39/200], Training Loss: 0.1412, Training Accuracy: 91.21%, Test Loss: 1.0023, Test Accuracy: 73.80%\n",
      "Epoch [40/200], Training Loss: 0.1163, Training Accuracy: 97.00%, Test Loss: 0.6953, Test Accuracy: 79.90%\n",
      "Epoch [41/200], Training Loss: 0.1129, Training Accuracy: 95.90%, Test Loss: 0.7195, Test Accuracy: 79.16%\n",
      "Epoch [42/200], Training Loss: 0.1355, Training Accuracy: 97.00%, Test Loss: 0.6176, Test Accuracy: 80.87%\n",
      "Epoch [43/200], Training Loss: 0.1205, Training Accuracy: 96.72%, Test Loss: 0.7392, Test Accuracy: 79.73%\n",
      "Epoch [44/200], Training Loss: 0.0764, Training Accuracy: 98.55%, Test Loss: 0.6173, Test Accuracy: 82.55%\n",
      "Epoch [45/200], Training Loss: 0.0685, Training Accuracy: 97.10%, Test Loss: 0.6919, Test Accuracy: 81.18%\n",
      "Epoch [46/200], Training Loss: 0.0824, Training Accuracy: 98.73%, Test Loss: 0.5827, Test Accuracy: 83.46%\n",
      "\n",
      "\n",
      "Test accuracy increased (82.8728 --> 83.4631). Saving model ...\n",
      "Epoch [47/200], Training Loss: 0.1224, Training Accuracy: 97.25%, Test Loss: 0.6897, Test Accuracy: 80.65%\n",
      "Epoch [48/200], Training Loss: 0.2141, Training Accuracy: 95.62%, Test Loss: 0.7965, Test Accuracy: 77.81%\n",
      "Epoch [49/200], Training Loss: 0.1749, Training Accuracy: 89.72%, Test Loss: 0.8920, Test Accuracy: 74.68%\n",
      "Epoch [50/200], Training Loss: 0.2971, Training Accuracy: 96.61%, Test Loss: 0.7143, Test Accuracy: 78.91%\n",
      "Epoch [51/200], Training Loss: 0.1778, Training Accuracy: 96.54%, Test Loss: 0.7281, Test Accuracy: 79.78%\n",
      "Epoch [52/200], Training Loss: 0.1073, Training Accuracy: 98.62%, Test Loss: 0.5741, Test Accuracy: 83.72%\n",
      "\n",
      "\n",
      "Test accuracy increased (83.4631 --> 83.7173). Saving model ...\n",
      "Epoch [53/200], Training Loss: 0.0496, Training Accuracy: 99.54%, Test Loss: 0.5652, Test Accuracy: 84.67%\n",
      "\n",
      "\n",
      "Test accuracy increased (83.7173 --> 84.6684). Saving model ...\n",
      "Epoch [54/200], Training Loss: 0.0307, Training Accuracy: 99.33%, Test Loss: 0.5597, Test Accuracy: 85.09%\n",
      "\n",
      "\n",
      "Test accuracy increased (84.6684 --> 85.0865). Saving model ...\n",
      "Epoch [55/200], Training Loss: 0.0305, Training Accuracy: 99.40%, Test Loss: 0.5722, Test Accuracy: 84.83%\n",
      "Epoch [56/200], Training Loss: 0.0547, Training Accuracy: 98.83%, Test Loss: 0.6364, Test Accuracy: 83.77%\n",
      "Epoch [57/200], Training Loss: 0.0442, Training Accuracy: 99.29%, Test Loss: 0.5854, Test Accuracy: 84.09%\n",
      "Epoch [58/200], Training Loss: 0.0491, Training Accuracy: 98.98%, Test Loss: 0.6764, Test Accuracy: 82.29%\n",
      "Epoch [59/200], Training Loss: 0.0489, Training Accuracy: 99.33%, Test Loss: 0.6390, Test Accuracy: 83.57%\n",
      "Epoch [60/200], Training Loss: 0.0822, Training Accuracy: 98.45%, Test Loss: 0.6822, Test Accuracy: 82.11%\n",
      "Epoch [61/200], Training Loss: 0.1569, Training Accuracy: 97.53%, Test Loss: 0.7570, Test Accuracy: 80.15%\n",
      "Epoch [62/200], Training Loss: 0.0879, Training Accuracy: 98.41%, Test Loss: 0.5826, Test Accuracy: 83.70%\n",
      "Epoch [63/200], Training Loss: 0.0759, Training Accuracy: 99.33%, Test Loss: 0.5805, Test Accuracy: 84.23%\n",
      "Epoch [64/200], Training Loss: 0.0419, Training Accuracy: 98.38%, Test Loss: 0.6469, Test Accuracy: 82.85%\n",
      "Epoch [65/200], Training Loss: 0.0490, Training Accuracy: 95.73%, Test Loss: 0.7782, Test Accuracy: 79.91%\n",
      "Epoch [66/200], Training Loss: 0.0796, Training Accuracy: 98.34%, Test Loss: 0.7378, Test Accuracy: 81.67%\n",
      "Epoch [67/200], Training Loss: 0.0723, Training Accuracy: 99.40%, Test Loss: 0.5884, Test Accuracy: 84.19%\n",
      "Epoch [68/200], Training Loss: 0.0816, Training Accuracy: 97.42%, Test Loss: 0.7611, Test Accuracy: 80.62%\n",
      "Epoch [69/200], Training Loss: 0.0611, Training Accuracy: 99.33%, Test Loss: 0.5746, Test Accuracy: 84.59%\n",
      "Epoch [70/200], Training Loss: 0.0608, Training Accuracy: 99.33%, Test Loss: 0.6054, Test Accuracy: 84.23%\n",
      "Epoch [71/200], Training Loss: 0.1463, Training Accuracy: 96.75%, Test Loss: 0.8225, Test Accuracy: 78.90%\n",
      "Epoch [72/200], Training Loss: 0.1581, Training Accuracy: 97.81%, Test Loss: 0.7006, Test Accuracy: 81.33%\n",
      "Epoch [73/200], Training Loss: 0.0563, Training Accuracy: 98.55%, Test Loss: 0.6502, Test Accuracy: 83.14%\n",
      "Epoch [74/200], Training Loss: 0.0478, Training Accuracy: 99.05%, Test Loss: 0.5873, Test Accuracy: 84.37%\n",
      "Epoch [75/200], Training Loss: 0.0862, Training Accuracy: 98.09%, Test Loss: 0.6577, Test Accuracy: 82.18%\n",
      "Epoch [76/200], Training Loss: 0.1085, Training Accuracy: 97.18%, Test Loss: 0.7687, Test Accuracy: 80.64%\n",
      "Epoch [77/200], Training Loss: 0.1168, Training Accuracy: 97.78%, Test Loss: 0.6622, Test Accuracy: 81.97%\n",
      "Epoch [78/200], Training Loss: 0.0700, Training Accuracy: 98.83%, Test Loss: 0.6667, Test Accuracy: 82.69%\n",
      "Epoch [79/200], Training Loss: 0.0687, Training Accuracy: 99.61%, Test Loss: 0.5407, Test Accuracy: 85.73%\n",
      "\n",
      "\n",
      "Test accuracy increased (85.0865 --> 85.7342). Saving model ...\n",
      "Epoch [80/200], Training Loss: 0.0422, Training Accuracy: 99.44%, Test Loss: 0.6268, Test Accuracy: 84.26%\n",
      "Epoch [81/200], Training Loss: 0.0464, Training Accuracy: 99.40%, Test Loss: 0.5848, Test Accuracy: 85.47%\n",
      "Epoch [82/200], Training Loss: 0.0276, Training Accuracy: 99.58%, Test Loss: 0.5757, Test Accuracy: 85.10%\n",
      "Epoch [83/200], Training Loss: 0.0859, Training Accuracy: 98.73%, Test Loss: 0.7148, Test Accuracy: 82.13%\n",
      "Epoch [84/200], Training Loss: 0.0991, Training Accuracy: 99.61%, Test Loss: 0.5671, Test Accuracy: 85.55%\n",
      "Epoch [85/200], Training Loss: 0.0426, Training Accuracy: 99.40%, Test Loss: 0.5899, Test Accuracy: 84.68%\n",
      "Epoch [86/200], Training Loss: 0.0542, Training Accuracy: 99.05%, Test Loss: 0.6837, Test Accuracy: 83.46%\n",
      "Epoch [87/200], Training Loss: 0.0601, Training Accuracy: 99.61%, Test Loss: 0.5770, Test Accuracy: 85.49%\n",
      "Epoch [88/200], Training Loss: 0.0243, Training Accuracy: 98.13%, Test Loss: 0.7470, Test Accuracy: 81.32%\n",
      "Epoch [89/200], Training Loss: 0.0485, Training Accuracy: 98.20%, Test Loss: 0.6517, Test Accuracy: 83.41%\n",
      "Epoch [90/200], Training Loss: 0.0516, Training Accuracy: 98.34%, Test Loss: 0.7003, Test Accuracy: 82.20%\n",
      "Epoch [91/200], Training Loss: 0.0664, Training Accuracy: 97.46%, Test Loss: 0.8449, Test Accuracy: 79.32%\n",
      "Epoch [92/200], Training Loss: 0.0649, Training Accuracy: 98.94%, Test Loss: 0.7032, Test Accuracy: 83.19%\n",
      "Epoch [93/200], Training Loss: 0.0375, Training Accuracy: 99.68%, Test Loss: 0.5533, Test Accuracy: 86.03%\n",
      "\n",
      "\n",
      "Test accuracy increased (85.7342 --> 86.0294). Saving model ...\n",
      "Epoch [94/200], Training Loss: 0.0249, Training Accuracy: 99.29%, Test Loss: 0.6773, Test Accuracy: 83.69%\n",
      "Epoch [95/200], Training Loss: 0.0224, Training Accuracy: 99.54%, Test Loss: 0.5942, Test Accuracy: 85.50%\n",
      "Epoch [96/200], Training Loss: 0.0175, Training Accuracy: 99.75%, Test Loss: 0.5692, Test Accuracy: 86.37%\n",
      "\n",
      "\n",
      "Test accuracy increased (86.0294 --> 86.3655). Saving model ...\n",
      "Epoch [97/200], Training Loss: 0.0116, Training Accuracy: 99.75%, Test Loss: 0.6129, Test Accuracy: 85.22%\n",
      "Epoch [98/200], Training Loss: 0.0061, Training Accuracy: 99.96%, Test Loss: 0.5203, Test Accuracy: 87.82%\n",
      "\n",
      "\n",
      "Test accuracy increased (86.3655 --> 87.8249). Saving model ...\n",
      "Epoch [99/200], Training Loss: 0.0059, Training Accuracy: 99.96%, Test Loss: 0.5632, Test Accuracy: 86.97%\n",
      "Epoch [100/200], Training Loss: 0.0066, Training Accuracy: 99.93%, Test Loss: 0.5707, Test Accuracy: 87.22%\n",
      "Epoch [101/200], Training Loss: 0.0154, Training Accuracy: 99.44%, Test Loss: 0.6456, Test Accuracy: 84.80%\n",
      "Epoch [102/200], Training Loss: 0.0385, Training Accuracy: 98.55%, Test Loss: 0.7050, Test Accuracy: 83.17%\n",
      "Epoch [103/200], Training Loss: 0.1799, Training Accuracy: 95.48%, Test Loss: 0.8483, Test Accuracy: 79.19%\n",
      "Epoch [104/200], Training Loss: 0.2324, Training Accuracy: 93.01%, Test Loss: 0.9863, Test Accuracy: 74.85%\n",
      "Epoch [105/200], Training Loss: 0.1974, Training Accuracy: 94.56%, Test Loss: 0.8881, Test Accuracy: 77.46%\n",
      "Epoch [106/200], Training Loss: 0.2112, Training Accuracy: 97.42%, Test Loss: 0.6803, Test Accuracy: 81.37%\n",
      "Epoch [107/200], Training Loss: 0.0849, Training Accuracy: 99.08%, Test Loss: 0.6413, Test Accuracy: 83.08%\n",
      "Epoch [108/200], Training Loss: 0.0557, Training Accuracy: 99.61%, Test Loss: 0.5508, Test Accuracy: 85.78%\n",
      "Epoch [109/200], Training Loss: 0.0501, Training Accuracy: 99.47%, Test Loss: 0.5630, Test Accuracy: 85.10%\n",
      "Epoch [110/200], Training Loss: 0.0604, Training Accuracy: 99.54%, Test Loss: 0.5397, Test Accuracy: 86.01%\n",
      "Epoch [111/200], Training Loss: 0.0370, Training Accuracy: 99.72%, Test Loss: 0.5758, Test Accuracy: 85.52%\n",
      "Epoch [112/200], Training Loss: 0.0199, Training Accuracy: 98.69%, Test Loss: 0.5752, Test Accuracy: 85.09%\n",
      "Epoch [113/200], Training Loss: 0.0201, Training Accuracy: 99.89%, Test Loss: 0.5768, Test Accuracy: 86.08%\n",
      "Epoch [114/200], Training Loss: 0.0082, Training Accuracy: 99.96%, Test Loss: 0.4804, Test Accuracy: 87.89%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.8249 --> 87.8905). Saving model ...\n",
      "Epoch [115/200], Training Loss: 0.0076, Training Accuracy: 99.89%, Test Loss: 0.5486, Test Accuracy: 86.91%\n",
      "Epoch [116/200], Training Loss: 0.0033, Training Accuracy: 99.93%, Test Loss: 0.4792, Test Accuracy: 88.58%\n",
      "\n",
      "\n",
      "Test accuracy increased (87.8905 --> 88.5792). Saving model ...\n",
      "Epoch [117/200], Training Loss: 0.0024, Training Accuracy: 99.93%, Test Loss: 0.4950, Test Accuracy: 88.43%\n",
      "Epoch [118/200], Training Loss: 0.0197, Training Accuracy: 99.54%, Test Loss: 0.5822, Test Accuracy: 86.46%\n",
      "Epoch [119/200], Training Loss: 0.0292, Training Accuracy: 98.59%, Test Loss: 0.7621, Test Accuracy: 82.50%\n",
      "Epoch [120/200], Training Loss: 0.0552, Training Accuracy: 99.61%, Test Loss: 0.5864, Test Accuracy: 85.66%\n",
      "Epoch [121/200], Training Loss: 0.0415, Training Accuracy: 98.45%, Test Loss: 0.6299, Test Accuracy: 84.23%\n",
      "Epoch [122/200], Training Loss: 0.0914, Training Accuracy: 97.99%, Test Loss: 0.7021, Test Accuracy: 83.00%\n",
      "Epoch [123/200], Training Loss: 0.0554, Training Accuracy: 99.22%, Test Loss: 0.6475, Test Accuracy: 84.29%\n",
      "Epoch [124/200], Training Loss: 0.0187, Training Accuracy: 99.54%, Test Loss: 0.5807, Test Accuracy: 85.88%\n",
      "Epoch [125/200], Training Loss: 0.0660, Training Accuracy: 98.31%, Test Loss: 0.7072, Test Accuracy: 83.00%\n",
      "Epoch [126/200], Training Loss: 0.0677, Training Accuracy: 98.76%, Test Loss: 0.6953, Test Accuracy: 82.95%\n",
      "Epoch [127/200], Training Loss: 0.0527, Training Accuracy: 96.79%, Test Loss: 0.8470, Test Accuracy: 80.02%\n",
      "Epoch [128/200], Training Loss: 0.0618, Training Accuracy: 98.62%, Test Loss: 0.6916, Test Accuracy: 83.41%\n",
      "Epoch [129/200], Training Loss: 0.1141, Training Accuracy: 98.59%, Test Loss: 0.6696, Test Accuracy: 83.20%\n",
      "Epoch [130/200], Training Loss: 0.0677, Training Accuracy: 97.81%, Test Loss: 0.7785, Test Accuracy: 81.47%\n",
      "Epoch [131/200], Training Loss: 0.1571, Training Accuracy: 92.34%, Test Loss: 0.8879, Test Accuracy: 77.42%\n",
      "Epoch [132/200], Training Loss: 0.1094, Training Accuracy: 98.94%, Test Loss: 0.6563, Test Accuracy: 83.45%\n",
      "Epoch [133/200], Training Loss: 0.0639, Training Accuracy: 97.74%, Test Loss: 0.6787, Test Accuracy: 82.91%\n",
      "Epoch [134/200], Training Loss: 0.0604, Training Accuracy: 99.15%, Test Loss: 0.6227, Test Accuracy: 85.00%\n",
      "Epoch [135/200], Training Loss: 0.0719, Training Accuracy: 97.25%, Test Loss: 0.8380, Test Accuracy: 79.65%\n",
      "Epoch [136/200], Training Loss: 0.0623, Training Accuracy: 99.54%, Test Loss: 0.5834, Test Accuracy: 85.58%\n",
      "Epoch [137/200], Training Loss: 0.0340, Training Accuracy: 97.92%, Test Loss: 0.7016, Test Accuracy: 82.97%\n",
      "Epoch [138/200], Training Loss: 0.0523, Training Accuracy: 99.75%, Test Loss: 0.5660, Test Accuracy: 86.47%\n",
      "Epoch [139/200], Training Loss: 0.0186, Training Accuracy: 99.86%, Test Loss: 0.5661, Test Accuracy: 86.93%\n",
      "Epoch [140/200], Training Loss: 0.0305, Training Accuracy: 99.51%, Test Loss: 0.5823, Test Accuracy: 86.17%\n",
      "Epoch [141/200], Training Loss: 0.0112, Training Accuracy: 99.89%, Test Loss: 0.5869, Test Accuracy: 86.68%\n",
      "Epoch [142/200], Training Loss: 0.0118, Training Accuracy: 99.93%, Test Loss: 0.4962, Test Accuracy: 87.98%\n",
      "Epoch [143/200], Training Loss: 0.0095, Training Accuracy: 99.96%, Test Loss: 0.5884, Test Accuracy: 86.59%\n",
      "Epoch [144/200], Training Loss: 0.0121, Training Accuracy: 99.96%, Test Loss: 0.5730, Test Accuracy: 86.79%\n",
      "Epoch [145/200], Training Loss: 0.0033, Training Accuracy: 100.00%, Test Loss: 0.4935, Test Accuracy: 88.69%\n",
      "\n",
      "\n",
      "Test accuracy increased (88.5792 --> 88.6939). Saving model ...\n",
      "Epoch [146/200], Training Loss: 0.0017, Training Accuracy: 100.00%, Test Loss: 0.5042, Test Accuracy: 88.55%\n",
      "Epoch [147/200], Training Loss: 0.0013, Training Accuracy: 100.00%, Test Loss: 0.4901, Test Accuracy: 89.03%\n",
      "\n",
      "\n",
      "Test accuracy increased (88.6939 --> 89.0301). Saving model ...\n",
      "Epoch [148/200], Training Loss: 0.0024, Training Accuracy: 99.93%, Test Loss: 0.5092, Test Accuracy: 88.58%\n",
      "Epoch [149/200], Training Loss: 0.0039, Training Accuracy: 99.93%, Test Loss: 0.5095, Test Accuracy: 88.39%\n",
      "Epoch [150/200], Training Loss: 0.0031, Training Accuracy: 99.93%, Test Loss: 0.5033, Test Accuracy: 88.60%\n",
      "Epoch [151/200], Training Loss: 0.0013, Training Accuracy: 100.00%, Test Loss: 0.5198, Test Accuracy: 88.32%\n",
      "Epoch [152/200], Training Loss: 0.0011, Training Accuracy: 100.00%, Test Loss: 0.5015, Test Accuracy: 88.67%\n",
      "Epoch [153/200], Training Loss: 0.0007, Training Accuracy: 100.00%, Test Loss: 0.4862, Test Accuracy: 89.11%\n",
      "\n",
      "\n",
      "Test accuracy increased (89.0301 --> 89.1121). Saving model ...\n",
      "Epoch [154/200], Training Loss: 0.0005, Training Accuracy: 100.00%, Test Loss: 0.4938, Test Accuracy: 88.98%\n",
      "Epoch [155/200], Training Loss: 0.0004, Training Accuracy: 100.00%, Test Loss: 0.4869, Test Accuracy: 89.16%\n",
      "\n",
      "\n",
      "Test accuracy increased (89.1121 --> 89.1613). Saving model ...\n",
      "Epoch [156/200], Training Loss: 0.0004, Training Accuracy: 100.00%, Test Loss: 0.4850, Test Accuracy: 89.21%\n",
      "\n",
      "\n",
      "Test accuracy increased (89.1613 --> 89.2105). Saving model ...\n",
      "Epoch [157/200], Training Loss: 0.0004, Training Accuracy: 100.00%, Test Loss: 0.4924, Test Accuracy: 89.10%\n",
      "Epoch [158/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.4984, Test Accuracy: 88.99%\n",
      "Epoch [159/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.4973, Test Accuracy: 89.06%\n",
      "Epoch [160/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.4940, Test Accuracy: 89.17%\n",
      "Epoch [161/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.4960, Test Accuracy: 89.15%\n",
      "Epoch [162/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.4971, Test Accuracy: 89.25%\n",
      "\n",
      "\n",
      "Test accuracy increased (89.2105 --> 89.2515). Saving model ...\n",
      "Epoch [163/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.4937, Test Accuracy: 89.30%\n",
      "\n",
      "\n",
      "Test accuracy increased (89.2515 --> 89.3006). Saving model ...\n",
      "Epoch [164/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.4876, Test Accuracy: 89.42%\n",
      "\n",
      "\n",
      "Test accuracy increased (89.3006 --> 89.4154). Saving model ...\n",
      "Epoch [165/200], Training Loss: 0.0003, Training Accuracy: 100.00%, Test Loss: 0.4930, Test Accuracy: 89.39%\n",
      "Epoch [166/200], Training Loss: 0.0002, Training Accuracy: 100.00%, Test Loss: 0.4876, Test Accuracy: 89.48%\n",
      "\n",
      "\n",
      "Test accuracy increased (89.4154 --> 89.4810). Saving model ...\n",
      "Epoch [167/200], Training Loss: 0.0004, Training Accuracy: 100.00%, Test Loss: 0.5024, Test Accuracy: 89.05%\n",
      "Epoch [168/200], Training Loss: 0.0044, Training Accuracy: 99.93%, Test Loss: 0.5159, Test Accuracy: 88.58%\n",
      "Epoch [169/200], Training Loss: 0.0173, Training Accuracy: 97.53%, Test Loss: 0.7251, Test Accuracy: 82.87%\n",
      "Epoch [170/200], Training Loss: 0.5366, Training Accuracy: 77.26%, Test Loss: 1.3295, Test Accuracy: 62.71%\n",
      "Epoch [171/200], Training Loss: 0.3570, Training Accuracy: 91.53%, Test Loss: 0.8019, Test Accuracy: 77.38%\n",
      "Epoch [172/200], Training Loss: 0.2127, Training Accuracy: 98.73%, Test Loss: 0.5891, Test Accuracy: 83.70%\n",
      "Epoch [173/200], Training Loss: 0.0681, Training Accuracy: 98.87%, Test Loss: 0.5594, Test Accuracy: 84.99%\n",
      "Epoch [174/200], Training Loss: 0.0450, Training Accuracy: 98.98%, Test Loss: 0.6065, Test Accuracy: 84.10%\n",
      "Epoch [175/200], Training Loss: 0.0340, Training Accuracy: 99.44%, Test Loss: 0.5351, Test Accuracy: 86.29%\n",
      "Epoch [176/200], Training Loss: 0.0387, Training Accuracy: 99.61%, Test Loss: 0.5568, Test Accuracy: 85.82%\n",
      "Epoch [177/200], Training Loss: 0.0226, Training Accuracy: 99.82%, Test Loss: 0.4997, Test Accuracy: 87.10%\n",
      "Epoch [178/200], Training Loss: 0.0204, Training Accuracy: 99.79%, Test Loss: 0.5132, Test Accuracy: 86.54%\n",
      "Epoch [179/200], Training Loss: 0.0170, Training Accuracy: 99.86%, Test Loss: 0.5466, Test Accuracy: 86.48%\n",
      "Epoch [180/200], Training Loss: 0.0447, Training Accuracy: 99.51%, Test Loss: 0.6184, Test Accuracy: 84.59%\n",
      "Epoch [181/200], Training Loss: 0.0495, Training Accuracy: 98.87%, Test Loss: 0.6837, Test Accuracy: 83.09%\n",
      "Epoch [182/200], Training Loss: 0.0933, Training Accuracy: 98.66%, Test Loss: 0.6996, Test Accuracy: 82.91%\n",
      "Epoch [183/200], Training Loss: 0.0575, Training Accuracy: 99.58%, Test Loss: 0.6340, Test Accuracy: 84.82%\n",
      "Epoch [184/200], Training Loss: 0.0705, Training Accuracy: 98.98%, Test Loss: 0.6788, Test Accuracy: 83.82%\n",
      "Epoch [185/200], Training Loss: 0.0771, Training Accuracy: 99.19%, Test Loss: 0.5921, Test Accuracy: 84.98%\n",
      "Epoch [186/200], Training Loss: 0.0343, Training Accuracy: 99.93%, Test Loss: 0.5412, Test Accuracy: 86.15%\n",
      "Epoch [187/200], Training Loss: 0.0273, Training Accuracy: 98.98%, Test Loss: 0.5859, Test Accuracy: 85.19%\n",
      "Epoch [188/200], Training Loss: 0.0475, Training Accuracy: 99.58%, Test Loss: 0.5754, Test Accuracy: 85.78%\n",
      "Epoch [189/200], Training Loss: 0.0460, Training Accuracy: 98.69%, Test Loss: 0.7232, Test Accuracy: 82.82%\n",
      "Epoch [190/200], Training Loss: 0.0395, Training Accuracy: 99.89%, Test Loss: 0.5432, Test Accuracy: 86.97%\n",
      "Epoch [191/200], Training Loss: 0.0102, Training Accuracy: 100.00%, Test Loss: 0.5383, Test Accuracy: 87.32%\n",
      "Epoch [192/200], Training Loss: 0.0268, Training Accuracy: 99.89%, Test Loss: 0.5941, Test Accuracy: 86.27%\n",
      "Epoch [193/200], Training Loss: 0.0228, Training Accuracy: 99.75%, Test Loss: 0.5403, Test Accuracy: 86.53%\n",
      "Epoch [194/200], Training Loss: 0.0194, Training Accuracy: 99.75%, Test Loss: 0.5996, Test Accuracy: 86.41%\n",
      "Epoch [195/200], Training Loss: 0.0374, Training Accuracy: 98.62%, Test Loss: 0.7101, Test Accuracy: 83.75%\n",
      "Epoch [196/200], Training Loss: 0.0654, Training Accuracy: 98.62%, Test Loss: 0.6596, Test Accuracy: 85.04%\n",
      "Epoch [197/200], Training Loss: 0.0459, Training Accuracy: 99.72%, Test Loss: 0.5851, Test Accuracy: 85.82%\n",
      "Epoch [198/200], Training Loss: 0.0257, Training Accuracy: 99.36%, Test Loss: 0.6747, Test Accuracy: 84.08%\n",
      "Epoch [199/200], Training Loss: 0.0720, Training Accuracy: 99.58%, Test Loss: 0.5723, Test Accuracy: 85.91%\n",
      "Epoch [200/200], Training Loss: 0.0388, Training Accuracy: 98.34%, Test Loss: 0.6816, Test Accuracy: 83.88%\n",
      "\n",
      "\n",
      "\n",
      "The train time (in seconds) is: 451.9452865123749\n",
      "Test Loss: 0.4876\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model.to(device)\n",
    "\n",
    "num_epochs = 200  # Set the number of epochs\n",
    "    \n",
    "KAPPA = []\n",
    "OA = []\n",
    "AA = []                \n",
    "ELEMENT_ACC = np.zeros((3, n_classes))\n",
    "\n",
    "for iterNum in range(3):\n",
    "    model = ViT_CA(in_channels=81, num_patches1=NC, num_patches2=NCLiDAR, dim=128, n_heads=8, dim_head=64, mlp_dim=256,  depth=3, num_classes=n_classes, dropout=0.1).cuda()\n",
    "    summary(model, [(81, NC, 1), (81, NCLiDAR, 1)])\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)  # Adjust learning rate as needed\n",
    "\n",
    "    # Define learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)  # Adjust parameters as needed\n",
    "\n",
    "    # Instantiate the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    best_test_acc = float('-inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training phase\n",
    "        for batch_idx, (hsi_batch, lidar_batch, label_batch) in enumerate(train_loader):\n",
    "            hsi_batch = hsi_batch.to(device)\n",
    "            lidar_batch = lidar_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(hsi_batch, lidar_batch)\n",
    "        \n",
    "            # Loss calculation\n",
    "            loss = criterion(outputs, label_batch)\n",
    "        \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            for hsi_batch, lidar_batch, label_batch in train_loader:\n",
    "                hsi_batch = hsi_batch.to(device)\n",
    "                lidar_batch = lidar_batch.to(device)\n",
    "                label_batch = label_batch.to(device)\n",
    "                outputs = model(hsi_batch, lidar_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_train += label_batch.size(0)\n",
    "                correct_train += (predicted == label_batch).sum().item()\n",
    "\n",
    "        train_accuracy = correct_train / total_train * 100\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        test_loss = 0.0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        with torch.no_grad():  # No need to track gradients during validation\n",
    "            for hsi_batch, lidar_batch, label_batch in test_loader:\n",
    "                hsi_batch = hsi_batch.to(device)\n",
    "                lidar_batch = lidar_batch.to(device)\n",
    "                label_batch = label_batch.to(device)\n",
    "\n",
    "                outputs = model(hsi_batch, lidar_batch)\n",
    "                loss = criterion(outputs, label_batch)\n",
    "                test_loss += loss.item()            \n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_test += label_batch.size(0)\n",
    "                correct_test += (predicted == label_batch).sum().item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        test_accuracy = correct_test / total_test * 100\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "        # Check if this is the best model so far\n",
    "        if test_accuracy > best_test_acc:\n",
    "            print('\\n')\n",
    "            print(f'Test accuracy increased ({best_test_acc:.4f} --> {test_accuracy:.4f}). Saving model ...')\n",
    "            best_test_acc = test_accuracy\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            # Save model weights\n",
    "            torch.save(model.state_dict(), 'best_model_'+str(filename)+'_Iter_'+str(iterNum)+'.pth')\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"\\n\")\n",
    "    print('\\nThe train time (in seconds) is:', end - start) \n",
    "                            \n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for hsi_batch, lidar_batch, label_batch in test_loader:\n",
    "            hsi_batch = hsi_batch.to(device)\n",
    "            lidar_batch = lidar_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "\n",
    "            outputs = model(hsi_batch, lidar_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            preds = torch.max(outputs, 1)[1].cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(label_batch.cpu().numpy())\n",
    "\n",
    "    # Calculate average test loss\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    print(f'Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "    # Convert lists to numpy arrays for evaluation metrics\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    get_confusion_matrix(all_labels, all_preds, datasetname, n_classes, filename + str('_') + str(iterNum))\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    oa = accuracy_score(all_labels, all_preds)\n",
    "    confusion = confusion_matrix(all_labels, all_preds)\n",
    "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
    "    kappa = cohen_kappa_score(all_labels, all_preds)    \n",
    "                    \n",
    "    KAPPA.append(kappa*100)\n",
    "    OA.append(oa*100)\n",
    "    AA.append(aa*100)\n",
    "    ELEMENT_ACC[iterNum, :] = each_acc*100\n",
    "    # torch.save(model, datasetname+'/best_model_'+filename+'_Iter'+str(iterNum)+'.pt')\n",
    "print(\"\\n\")\n",
    "record.record_output(OA, AA, KAPPA, ELEMENT_ACC,'./' + datasetname+'/'+filename+'_Report_' + datasetname +'.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e0bab-b64b-4e2f-92d2-a925cf873876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srinadh_astro",
   "language": "python",
   "name": "srinadh_astro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "26407125db06bdd9abe40e82cf041582bb19887fa16dd38638e528b7039723e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
